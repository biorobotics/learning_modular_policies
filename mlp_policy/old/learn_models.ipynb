{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using folder learned_models\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Learn models with different options, for comparison plots\n",
    "\n",
    "'''\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from robot_env import robot_env\n",
    "import numpy as np\n",
    "from pmlp import pmlp\n",
    "import pgnn\n",
    "from utils import get_sampleable_inds, sample_memory\n",
    "from utils import to_body_frame_batch, from_body_frame_batch\n",
    "from utils import state_diff_batch, state_to_fd_input, state_add_batch\n",
    "from utils import divide_state, to_device, detach_list, clip_grads\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "# cwd = os.path.dirname(os.path.realpath(__file__))\n",
    "cwd = ''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "folder = os.path.join(cwd, 'learned_models')\n",
    "if not(os.path.exists(folder)):\n",
    "    os.mkdir(folder)\n",
    "    print('Created folder ' + folder )\n",
    "else:\n",
    "    print('Using folder '  + folder)\n",
    "    \n",
    "start_time = datetime.now()\n",
    "start_time_str = datetime.strftime(start_time, '%Y%m%d_%H%M')\n",
    "\n",
    "RUN_MLP = True\n",
    "RUN_SINGLE_GNN = True\n",
    "RUN_MULTI_GNN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attachments: [[1, None, 2, 3, None, 4], [0], [0], [0], [0]]\n",
      "modules_types: [0, 2, 2, 2, 2]\n",
      "Delta fd len: 24\n",
      "attachments: [[1, 2, 3, 4, 5, 6], [0], [0], [0], [0], [0], [0]]\n",
      "modules_types: [0, 1, 1, 1, 1, 1, 1]\n",
      "Delta fd len: 48\n",
      "attachments: [[1, 2, 3, 4, 5, 6], [0], [0], [0], [0], [0], [0]]\n",
      "modules_types: [0, 1, 1, 2, 2, 1, 1]\n",
      "Delta fd len: 42\n"
     ]
    }
   ],
   "source": [
    "# load dataset and gather module configs\n",
    "\n",
    "urdf_names = ['wnwwnw', 'llllll', 'llwwll']\n",
    "# urdf_names = ['llllll', 'llwwll', \n",
    "# 'lwllwl', 'lwwwwl', \n",
    "# 'lnllnl', 'lnwwnl',\n",
    "# 'wlwwlw',  'wwllww', \n",
    "# 'wwwwww',  'wnllnw', \n",
    "#  'wllllw', 'wnwwnw']\n",
    "\n",
    "envs = dict()\n",
    "run_lens = dict()\n",
    "states_memory_tensors = dict()\n",
    "actions_memory_tensors = dict()\n",
    "module_sa_len = dict()\n",
    "modules_types = dict()\n",
    "attachments = dict()\n",
    "delta_fd_len = dict()\n",
    "\n",
    "for urdf in urdf_names:\n",
    "\n",
    "    env = robot_env(show_GUI = False)\n",
    "    env.reset_terrain()\n",
    "    env.reset_robot(urdf_name=urdf, randomize_start=False)\n",
    "    attachments[urdf] = env.attachments\n",
    "    modules_types[urdf] = env.modules_types\n",
    "    print('attachments: ' + str(attachments[urdf]))\n",
    "    print('modules_types: ' + str(modules_types[urdf]))\n",
    "    n_modules = len(modules_types[urdf])\n",
    "    envs[urdf] = env\n",
    "    state = env.get_state()\n",
    "    state_t = [torch.tensor(s, dtype=torch.float32).unsqueeze(0) for s in state]\n",
    "    module_state_len = []\n",
    "    for s in state:\n",
    "        module_state_len.append(len(s))\n",
    "    module_action_len = list(np.diff(env.action_indexes))\n",
    "    state_len = sum(module_state_len)\n",
    "    module_sa_len[urdf] = module_state_len+ module_action_len\n",
    "    \n",
    "#     fd_input_test, delta_fd_test = to_body_frame_batch(state_t, state_t)\n",
    "#     delta_fd_len[urdf] = torch.cat(delta_fd_test,-1).shape[-1]\n",
    "    delta_fd_len[urdf] = sum(module_sa_len[urdf][0:len(modules_types[urdf])])\n",
    "    print('Delta fd len:' , delta_fd_len[urdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files \n",
      "['random_rollouts/wnwwnw_random_rollouts.ptx']\n",
      "loading random_rollouts/wnwwnw_random_rollouts.ptx\n",
      "Found files \n",
      "['random_rollouts/llllll_random_rollouts.ptx']\n",
      "loading random_rollouts/llllll_random_rollouts.ptx\n",
      "loaded and merged data\n"
     ]
    }
   ],
   "source": [
    "for urdf in urdf_names:\n",
    "    # load dataset\n",
    "    file_names = []\n",
    "    folder = 'random_rollouts/'\n",
    "    found = True\n",
    "    fname_test = os.path.join(cwd, folder+urdf+'_random_rollouts.ptx')\n",
    "    if os.path.isfile(fname_test):\n",
    "        file_names.append(fname_test)\n",
    "#     f_index = 1\n",
    "#     while found:\n",
    "#         fname_test = os.path.join(cwd,\n",
    "#             folder+urdf+'_rollouts' + str(int(f_index)) + '.ptx')\n",
    "#         found = os.path.isfile(fname_test)\n",
    "#         if found: \n",
    "#             file_names.append(fname_test)\n",
    "#             f_index+=1\n",
    "    print('Found files ')\n",
    "    print(str(file_names))\n",
    "\n",
    "    states_memory = []\n",
    "    actions_memory = []\n",
    "    run_lens[urdf] = []\n",
    "\n",
    "    for fname in file_names:\n",
    "        print('loading ' + fname )\n",
    "        data_in = torch.load(fname)\n",
    "        states_memory += data_in['states_memory']\n",
    "        actions_memory += data_in['actions_memory']\n",
    "        run_lens[urdf] += data_in['run_lens']\n",
    "        del data_in\n",
    "\n",
    "    states_memory_tensors[urdf] = [torch.cat(s,0) for s in list(zip(*states_memory)) ]\n",
    "    actions_memory_tensors[urdf] = [torch.cat(s,0) for s in list(zip(*actions_memory)) ]\n",
    "\n",
    "print('loaded and merged data')\n",
    "    \n",
    "batch_size_default = 500 # default batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder random_rollouts/runs/20200710_1645_100_2\n",
      "Created folder random_rollouts/runs/20200710_1645_1000_2\n",
      "Created folder random_rollouts/runs/20200710_1645_5000_2\n"
     ]
    }
   ],
   "source": [
    "# urdf_to_train = urdf_names[1]\n",
    "\n",
    "# select options:\n",
    "n_training_steps = 30000\n",
    "\n",
    "condition_tuples = [(100, 2),\n",
    "                    (1000, 2),\n",
    "                    (5000, 2)]\n",
    "\n",
    "#                     (100, 10),\n",
    "#                     (1000, 10),\n",
    "#                     (5000, 10)]\n",
    "\n",
    "# # select data regime (number of data samples to use, low mid or high)\n",
    "# n_rollouts_to_use = 100 # Low\n",
    "# n_rollouts_to_use = 1000 # mid\n",
    "# n_rollouts_to_use = 5000 # high\n",
    "\n",
    "# # select sequence length for multistep loss\n",
    "# seq_len = 10\n",
    "# # seq_len = 2\n",
    "\n",
    "for condition_tuple in condition_tuples:\n",
    "    n_rollouts_to_use = condition_tuple[0]\n",
    "    seq_len = condition_tuple[1]\n",
    "    condition_run_folder = os.path.join(folder,'runs', start_time_str+ '_' + str(n_rollouts_to_use) + '_' + str(seq_len))\n",
    "    if not(os.path.exists(condition_run_folder)):\n",
    "        os.mkdir(condition_run_folder)\n",
    "        print('Created folder ' + condition_run_folder )\n",
    "    else:\n",
    "        print('Using folder '  + condition_run_folder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running condition: n_rollouts=100seq_len=2\n",
      "wnwwnw using 100 out of Rollouts 5000\n",
      "Num NN params: 790446\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fd_input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-271cbee599b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                             str(np.round(loss_np,2)))\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfd_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fd_input' is not defined"
     ]
    }
   ],
   "source": [
    "# train P-MLP\n",
    "\n",
    "\n",
    "# urdf = urdf_to_train\n",
    "if RUN_MLP:    \n",
    "    for urdf in urdf_names:\n",
    "\n",
    "        for condition_tuple in condition_tuples:\n",
    "            n_rollouts_to_use = condition_tuple[0]\n",
    "            seq_len = condition_tuple[1]\n",
    "\n",
    "            print('--- Running condition: ' \n",
    "                  + 'n_rollouts=' + str(n_rollouts_to_use)\n",
    "                  + 'seq_len=' + str(seq_len))\n",
    "\n",
    "            comment_str = '_pmlp_' + urdf + str(n_rollouts_to_use) + '_' + str(seq_len)\n",
    "            writer = SummaryWriter(log_dir = os.path.join(folder,'runs',\n",
    "                        start_time_str+ '_' + str(n_rollouts_to_use) + '_' + str(seq_len)),\n",
    "                        comment=comment_str)\n",
    "\n",
    "            # depending on the length of the multistep sequence we want,\n",
    "            # only some indexes of the full set of states collected can be sampled.\n",
    "            sampleable_inds = dict()\n",
    "            batch_sizes = dict()\n",
    "        #     for urdf in urdf_names:\n",
    "\n",
    "            sampleable_inds[urdf] = get_sampleable_inds(\n",
    "                run_lens[urdf][:n_rollouts_to_use], seq_len)\n",
    "            n_sampleable = len(sampleable_inds[urdf])\n",
    "            batch_sizes[urdf] = batch_size_default\n",
    "            if batch_sizes[urdf] > n_sampleable:\n",
    "                batch_sizes[urdf] = n_sampleable\n",
    "            print(urdf + ' using ' + str(n_rollouts_to_use) + ' out of Rollouts ' + str(len(run_lens[urdf])))\n",
    "\n",
    "\n",
    "\n",
    "            batch_size = batch_sizes[urdf]\n",
    "            n_modules = len(modules_types[urdf])\n",
    "            module_state_len = module_sa_len[urdf][:n_modules]\n",
    "            # initialize network and optimizer\n",
    "            input_len = sum(module_sa_len[urdf]) - 3\n",
    "            output_len = sum(module_state_len)\n",
    "            hidden_layer_size = 300\n",
    "            n_hidden_layers = 5\n",
    "            fd_network = pmlp(input_len = input_len, output_len=output_len,\n",
    "                n_hidden_layers = n_hidden_layers, hidden_layer_size=hidden_layer_size\n",
    "                ).to(device)\n",
    "            weight_decay = 1e-4\n",
    "            optimizer =  torch.optim.Adam(fd_network.parameters(),lr=1e-3, weight_decay = weight_decay) \n",
    "\n",
    "            num_nn_params=0\n",
    "            for p in fd_network.parameters():\n",
    "                nn=1\n",
    "                for s in list(p.size()):\n",
    "                    nn = nn*s\n",
    "                num_nn_params += nn\n",
    "            print('Num NN params: ' + str(num_nn_params))\n",
    "\n",
    "            for training_step in range( n_training_steps):\n",
    "    #         for training_step in range(0):\n",
    "\n",
    "                if np.mod(training_step,5000 )==0 and training_step>10000:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = param_group['lr']/2\n",
    "                        print( 'LR: ' + str(param_group['lr']) )\n",
    "\n",
    "                # sample without replacement from the full memory, depending on what is sampleable\n",
    "                state_seq, action_seq, sampled_inds = sample_memory(\n",
    "                                states_memory_tensors[urdf], \n",
    "                                actions_memory_tensors[urdf],\n",
    "                                sampleable_inds[urdf], seq_len, batch_size)\n",
    "\n",
    "                loss = 0\n",
    "                state_approx = to_device(state_seq[0],device) # initial state input is the first in sequence\n",
    "                for seq in range(seq_len-1): # for multistep loss, go through the sequence\n",
    "\n",
    "                    # process states to move them to vehicle frame\n",
    "                    fd_input_real, delta_fd_real = to_body_frame_batch(state_seq[seq], state_seq[seq+1])\n",
    "                    fd_input_approx, R_t = state_to_fd_input(state_approx) # for recursive estimation\n",
    "\n",
    "                    # pass through network\n",
    "                    fd_input = torch.cat(fd_input_approx,1).to(device)\n",
    "                    actions_in = torch.cat(action_seq[seq],1).to(device)\n",
    "                    delta_fd = torch.cat(delta_fd_real,1).to(device)\n",
    "                    state_delta_est_mean, state_delta_est_var = fd_network(fd_input, actions_in)\n",
    "\n",
    "                    # compute loss for this step in sequence\n",
    "                    loss += torch.sum(\n",
    "                         (state_delta_est_mean - delta_fd)**2/state_delta_est_var\n",
    "                         + torch.log(state_delta_est_var)\n",
    "                         )/batch_size/(seq_len-1)   \n",
    "\n",
    "                    # transform back to world frame advance to next sequence step\n",
    "                    if seq_len>2:\n",
    "                        # divide MLP output divided up into modules\n",
    "                        delta_fd_approx = divide_state(state_delta_est_mean, module_state_len)\n",
    "                        # update recursive state estimation for multistep loss  \n",
    "                        state_approx = from_body_frame_batch(state_approx, delta_fd_approx)\n",
    "\n",
    "\n",
    "                # backprop and optimizer step \n",
    "                loss_np = loss.detach().cpu().numpy()\n",
    "                fd_network.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                writer.add_scalar('Train' + '/Loss_pmlp', loss_np, training_step)\n",
    "\n",
    "\n",
    "                # periodically save the model\n",
    "\n",
    "                if np.mod(training_step,500)==0:\n",
    "                    PATH = ('learned_models/' + urdf + '_pmlp_r' + str(int(n_rollouts_to_use)) + \n",
    "                            '_ms'+ str(int(seq_len))+'.pt')\n",
    "                    PATH = os.path.join(cwd, PATH)\n",
    "                    fd_network_state_dict=fd_network.state_dict()\n",
    "                    torch.save({'fd_network_state_dict':fd_network_state_dict,\n",
    "                        'fd_network_input_len':fd_network.input_len,\n",
    "                        'fd_network_output_len':fd_network.output_len,\n",
    "                        'fd_network_n_hidden_layers':fd_network.n_hidden_layers,\n",
    "                        'fd_network_hidden_layer_size':fd_network.hidden_layer_size,\n",
    "                        'n_rollouts_to_use':n_rollouts_to_use,\n",
    "                        'seq_len':seq_len,\n",
    "                        'batch_size':batch_size,\n",
    "                        'urdf':urdf,\n",
    "                        'num_nn_params':num_nn_params,\n",
    "                        'weight_decay':weight_decay\n",
    "                        },  PATH)  \n",
    "                    print('Training losses at iter ' + \n",
    "                            str(training_step) + ': ' + \n",
    "                            str(np.round(loss_np,2)))\n",
    "            del fd_input, actions_in, delta_fd, fd_network, loss, optimizer\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wnwwnw using 100 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 15.45\n",
      "Training losses at iter 500: -77.04\n",
      "Training losses at iter 1000: -94.33\n",
      "Training losses at iter 1500: -107.5\n",
      "Training losses at iter 2000: -105.14\n",
      "Training losses at iter 2500: -117.26\n",
      "Training losses at iter 3000: -130.7\n",
      "Training losses at iter 3500: -124.73\n",
      "Training losses at iter 4000: -125.47\n",
      "Training losses at iter 4500: -139.18\n",
      "Training losses at iter 5000: -132.45\n",
      "Training losses at iter 5500: -146.93\n",
      "Training losses at iter 6000: -149.89\n",
      "Training losses at iter 6500: -155.27\n",
      "Training losses at iter 7000: -160.98\n",
      "Training losses at iter 7500: -164.75\n",
      "Training losses at iter 8000: -165.3\n",
      "Training losses at iter 8500: -170.19\n",
      "Training losses at iter 9000: -165.68\n",
      "Training losses at iter 9500: -168.99\n",
      "Training losses at iter 10000: -172.62\n",
      "Training losses at iter 10500: -180.19\n",
      "Training losses at iter 11000: -180.25\n",
      "Training losses at iter 11500: -182.58\n",
      "Training losses at iter 12000: -182.91\n",
      "Training losses at iter 12500: -188.16\n",
      "Training losses at iter 13000: -184.6\n",
      "Training losses at iter 13500: -188.9\n",
      "Training losses at iter 14000: -189.18\n",
      "Training losses at iter 14500: -168.47\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -193.44\n",
      "Training losses at iter 15500: -195.61\n",
      "Training losses at iter 16000: -198.9\n",
      "Training losses at iter 16500: -199.24\n",
      "Training losses at iter 17000: -205.27\n",
      "Training losses at iter 17500: -200.7\n",
      "Training losses at iter 18000: -204.02\n",
      "Training losses at iter 18500: -202.26\n",
      "Training losses at iter 19000: -206.54\n",
      "Training losses at iter 19500: -205.28\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -210.0\n",
      "Training losses at iter 20500: -215.28\n",
      "Training losses at iter 21000: -213.77\n",
      "Training losses at iter 21500: -217.24\n",
      "Training losses at iter 22000: -217.47\n",
      "Training losses at iter 22500: -217.46\n",
      "Training losses at iter 23000: -220.31\n",
      "Training losses at iter 23500: -219.74\n",
      "Training losses at iter 24000: -219.31\n",
      "Training losses at iter 24500: -217.56\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -218.59\n",
      "Training losses at iter 25500: -224.16\n",
      "Training losses at iter 26000: -224.75\n",
      "Training losses at iter 26500: -222.88\n",
      "Training losses at iter 27000: -226.23\n",
      "Training losses at iter 27500: -222.62\n",
      "Training losses at iter 28000: -224.17\n",
      "Training losses at iter 28500: -225.39\n",
      "Training losses at iter 29000: -227.42\n",
      "Training losses at iter 29500: -225.97\n",
      "wnwwnw using 1000 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 17.7\n",
      "Training losses at iter 500: -74.72\n",
      "Training losses at iter 1000: -76.31\n",
      "Training losses at iter 1500: -94.32\n",
      "Training losses at iter 2000: -94.8\n",
      "Training losses at iter 2500: -113.96\n",
      "Training losses at iter 3000: -115.53\n",
      "Training losses at iter 3500: -126.41\n",
      "Training losses at iter 4000: -132.18\n",
      "Training losses at iter 4500: -128.56\n",
      "Training losses at iter 5000: -126.68\n",
      "Training losses at iter 5500: -138.53\n",
      "Training losses at iter 6000: -144.08\n",
      "Training losses at iter 6500: -142.9\n",
      "Training losses at iter 7000: -142.92\n",
      "Training losses at iter 7500: -145.86\n",
      "Training losses at iter 8000: -148.94\n",
      "Training losses at iter 8500: -150.66\n",
      "Training losses at iter 9000: -151.43\n",
      "Training losses at iter 9500: -148.24\n",
      "Training losses at iter 10000: -149.02\n",
      "Training losses at iter 10500: -154.59\n",
      "Training losses at iter 11000: -157.65\n",
      "Training losses at iter 11500: -160.94\n",
      "Training losses at iter 12000: -159.86\n",
      "Training losses at iter 12500: -159.77\n",
      "Training losses at iter 13000: -152.22\n",
      "Training losses at iter 13500: -161.43\n",
      "Training losses at iter 14000: -161.64\n",
      "Training losses at iter 14500: -162.79\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -159.44\n",
      "Training losses at iter 15500: -173.31\n",
      "Training losses at iter 16000: -171.68\n",
      "Training losses at iter 16500: -171.61\n",
      "Training losses at iter 17000: -170.32\n",
      "Training losses at iter 17500: -170.89\n",
      "Training losses at iter 18000: -172.51\n",
      "Training losses at iter 18500: -174.24\n",
      "Training losses at iter 19000: -168.95\n",
      "Training losses at iter 19500: -168.42\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -173.08\n",
      "Training losses at iter 20500: -173.65\n",
      "Training losses at iter 21000: -177.64\n",
      "Training losses at iter 21500: -176.78\n",
      "Training losses at iter 22000: -177.27\n",
      "Training losses at iter 22500: -178.79\n",
      "Training losses at iter 23000: -176.49\n",
      "Training losses at iter 23500: -178.31\n",
      "Training losses at iter 24000: -178.63\n",
      "Training losses at iter 24500: -178.8\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -179.44\n",
      "Training losses at iter 25500: -181.23\n",
      "Training losses at iter 26000: -181.58\n",
      "Training losses at iter 26500: -181.31\n",
      "Training losses at iter 27000: -182.21\n",
      "Training losses at iter 27500: -181.32\n",
      "Training losses at iter 28000: -179.52\n",
      "Training losses at iter 28500: -181.67\n",
      "Training losses at iter 29000: -179.83\n",
      "Training losses at iter 29500: -181.23\n",
      "wnwwnw using 5000 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 15.15\n",
      "Training losses at iter 500: -68.77\n",
      "Training losses at iter 1000: -83.67\n",
      "Training losses at iter 1500: -75.39\n",
      "Training losses at iter 2000: -94.61\n",
      "Training losses at iter 2500: -110.35\n",
      "Training losses at iter 3000: -112.19\n",
      "Training losses at iter 3500: -115.74\n",
      "Training losses at iter 4000: -118.46\n",
      "Training losses at iter 4500: -122.02\n",
      "Training losses at iter 5000: -124.23\n",
      "Training losses at iter 5500: -129.32\n",
      "Training losses at iter 6000: -137.16\n",
      "Training losses at iter 6500: -136.07\n",
      "Training losses at iter 7000: -129.74\n",
      "Training losses at iter 7500: -135.1\n",
      "Training losses at iter 8000: -141.49\n",
      "Training losses at iter 8500: -134.45\n",
      "Training losses at iter 9000: -147.12\n",
      "Training losses at iter 9500: -137.18\n",
      "Training losses at iter 10000: -146.37\n",
      "Training losses at iter 10500: -137.98\n",
      "Training losses at iter 11000: -125.12\n",
      "Training losses at iter 11500: -148.36\n",
      "Training losses at iter 12000: -147.03\n",
      "Training losses at iter 12500: -149.81\n",
      "Training losses at iter 13000: -150.45\n",
      "Training losses at iter 13500: -149.24\n",
      "Training losses at iter 14000: -141.28\n",
      "Training losses at iter 14500: -152.52\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -139.95\n",
      "Training losses at iter 15500: -158.4\n",
      "Training losses at iter 16000: -157.19\n",
      "Training losses at iter 16500: -159.06\n",
      "Training losses at iter 17000: -160.24\n",
      "Training losses at iter 17500: -160.54\n",
      "Training losses at iter 18000: -158.85\n",
      "Training losses at iter 18500: -162.24\n",
      "Training losses at iter 19000: -164.7\n",
      "Training losses at iter 19500: -161.64\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -159.64\n",
      "Training losses at iter 20500: -170.01\n",
      "Training losses at iter 21000: -166.64\n",
      "Training losses at iter 21500: -167.79\n",
      "Training losses at iter 22000: -165.62\n",
      "Training losses at iter 22500: -167.9\n",
      "Training losses at iter 23000: -163.32\n",
      "Training losses at iter 23500: -169.79\n",
      "Training losses at iter 24000: -165.87\n",
      "Training losses at iter 24500: -169.09\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -169.19\n",
      "Training losses at iter 25500: -173.92\n",
      "Training losses at iter 26000: -170.42\n",
      "Training losses at iter 26500: -171.79\n",
      "Training losses at iter 27000: -171.95\n",
      "Training losses at iter 27500: -170.06\n",
      "Training losses at iter 28000: -174.85\n",
      "Training losses at iter 28500: -172.68\n",
      "Training losses at iter 29000: -167.16\n",
      "Training losses at iter 29500: -172.91\n",
      "llllll using 100 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 4.66\n",
      "Training losses at iter 500: -152.41\n",
      "Training losses at iter 1000: -169.71\n",
      "Training losses at iter 1500: -188.36\n",
      "Training losses at iter 2000: -204.33\n",
      "Training losses at iter 2500: -218.2\n",
      "Training losses at iter 3000: -223.76\n",
      "Training losses at iter 3500: -233.45\n",
      "Training losses at iter 4000: -250.19\n",
      "Training losses at iter 4500: -239.77\n",
      "Training losses at iter 5000: -262.11\n",
      "Training losses at iter 5500: -256.54\n",
      "Training losses at iter 6000: -277.76\n",
      "Training losses at iter 6500: -288.68\n",
      "Training losses at iter 7000: -289.47\n",
      "Training losses at iter 7500: -301.45\n",
      "Training losses at iter 8000: -245.32\n",
      "Training losses at iter 8500: -304.34\n",
      "Training losses at iter 9000: -302.4\n",
      "Training losses at iter 9500: -304.14\n",
      "Training losses at iter 10000: -318.92\n",
      "Training losses at iter 10500: -322.55\n",
      "Training losses at iter 11000: -290.5\n",
      "Training losses at iter 11500: -326.19\n",
      "Training losses at iter 12000: -318.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses at iter 12500: -322.71\n",
      "Training losses at iter 13000: -327.61\n",
      "Training losses at iter 13500: -328.43\n",
      "Training losses at iter 14000: -291.37\n",
      "Training losses at iter 14500: -345.26\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -349.55\n",
      "Training losses at iter 15500: -362.92\n",
      "Training losses at iter 16000: -369.92\n",
      "Training losses at iter 16500: -371.85\n",
      "Training losses at iter 17000: -366.89\n",
      "Training losses at iter 17500: -373.8\n",
      "Training losses at iter 18000: -379.6\n",
      "Training losses at iter 18500: -372.13\n",
      "Training losses at iter 19000: -378.35\n",
      "Training losses at iter 19500: -383.72\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -374.93\n",
      "Training losses at iter 20500: -393.7\n",
      "Training losses at iter 21000: -394.52\n",
      "Training losses at iter 21500: -398.46\n",
      "Training losses at iter 22000: -398.65\n",
      "Training losses at iter 22500: -398.8\n",
      "Training losses at iter 23000: -395.93\n",
      "Training losses at iter 23500: -404.64\n",
      "Training losses at iter 24000: -398.24\n",
      "Training losses at iter 24500: -406.12\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -403.78\n",
      "Training losses at iter 25500: -415.7\n",
      "Training losses at iter 26000: -411.57\n",
      "Training losses at iter 26500: -411.15\n",
      "Training losses at iter 27000: -411.69\n",
      "Training losses at iter 27500: -407.88\n",
      "Training losses at iter 28000: -417.81\n",
      "Training losses at iter 28500: -411.43\n",
      "Training losses at iter 29000: -413.11\n",
      "Training losses at iter 29500: -417.45\n",
      "llllll using 1000 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 4.75\n",
      "Training losses at iter 500: -152.68\n",
      "Training losses at iter 1000: -170.71\n",
      "Training losses at iter 1500: -184.37\n",
      "Training losses at iter 2000: -173.73\n",
      "Training losses at iter 2500: -200.41\n",
      "Training losses at iter 3000: -203.59\n",
      "Training losses at iter 3500: -207.85\n",
      "Training losses at iter 4000: -223.98\n",
      "Training losses at iter 4500: -230.34\n",
      "Training losses at iter 5000: -231.6\n",
      "Training losses at iter 5500: -230.38\n",
      "Training losses at iter 6000: -227.22\n",
      "Training losses at iter 6500: -239.38\n",
      "Training losses at iter 7000: -242.36\n",
      "Training losses at iter 7500: -242.41\n",
      "Training losses at iter 8000: -245.57\n",
      "Training losses at iter 8500: -256.98\n",
      "Training losses at iter 9000: -256.63\n",
      "Training losses at iter 9500: -253.93\n",
      "Training losses at iter 10000: -261.69\n",
      "Training losses at iter 10500: -262.27\n",
      "Training losses at iter 11000: -265.99\n",
      "Training losses at iter 11500: -261.13\n",
      "Training losses at iter 12000: -263.64\n",
      "Training losses at iter 12500: -266.95\n",
      "Training losses at iter 13000: -270.05\n",
      "Training losses at iter 13500: -275.56\n",
      "Training losses at iter 14000: -278.99\n",
      "Training losses at iter 14500: -269.85\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -282.33\n",
      "Training losses at iter 15500: -288.22\n",
      "Training losses at iter 16000: -287.41\n",
      "Training losses at iter 16500: -292.91\n",
      "Training losses at iter 17000: -293.22\n",
      "Training losses at iter 17500: -300.13\n",
      "Training losses at iter 18000: -299.23\n",
      "Training losses at iter 18500: -296.91\n",
      "Training losses at iter 19000: -296.73\n",
      "Training losses at iter 19500: -300.2\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -298.83\n",
      "Training losses at iter 20500: -303.67\n",
      "Training losses at iter 21000: -306.02\n",
      "Training losses at iter 21500: -305.13\n",
      "Training losses at iter 22000: -307.36\n",
      "Training losses at iter 22500: -304.61\n",
      "Training losses at iter 23000: -308.59\n",
      "Training losses at iter 23500: -309.16\n",
      "Training losses at iter 24000: -313.34\n",
      "Training losses at iter 24500: -306.8\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -308.44\n",
      "Training losses at iter 25500: -310.0\n",
      "Training losses at iter 26000: -310.37\n",
      "Training losses at iter 26500: -312.82\n",
      "Training losses at iter 27000: -314.08\n",
      "Training losses at iter 27500: -313.16\n",
      "Training losses at iter 28000: -311.25\n",
      "Training losses at iter 28500: -312.64\n",
      "Training losses at iter 29000: -314.44\n",
      "Training losses at iter 29500: -317.97\n",
      "llllll using 5000 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 3.19\n",
      "Training losses at iter 500: -153.09\n",
      "Training losses at iter 1000: -178.58\n",
      "Training losses at iter 1500: -183.7\n",
      "Training losses at iter 2000: -184.74\n",
      "Training losses at iter 2500: -199.8\n",
      "Training losses at iter 3000: -202.06\n",
      "Training losses at iter 3500: -205.97\n",
      "Training losses at iter 4000: -200.69\n",
      "Training losses at iter 4500: -221.06\n",
      "Training losses at iter 5000: -222.05\n",
      "Training losses at iter 5500: -229.74\n",
      "Training losses at iter 6000: -222.19\n",
      "Training losses at iter 6500: -232.52\n",
      "Training losses at iter 7000: -236.46\n",
      "Training losses at iter 7500: -243.75\n",
      "Training losses at iter 8000: -238.94\n",
      "Training losses at iter 8500: -246.99\n",
      "Training losses at iter 9000: -248.88\n",
      "Training losses at iter 9500: -254.08\n",
      "Training losses at iter 10000: -247.47\n",
      "Training losses at iter 10500: -254.42\n",
      "Training losses at iter 11000: -255.3\n",
      "Training losses at iter 11500: -260.68\n",
      "Training losses at iter 12000: -259.71\n",
      "Training losses at iter 12500: -266.43\n",
      "Training losses at iter 13000: -260.25\n",
      "Training losses at iter 13500: -261.81\n",
      "Training losses at iter 14000: -266.62\n",
      "Training losses at iter 14500: -259.42\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -264.97\n",
      "Training losses at iter 15500: -259.11\n",
      "Training losses at iter 16000: -272.26\n",
      "Training losses at iter 16500: -273.88\n",
      "Training losses at iter 17000: -276.55\n",
      "Training losses at iter 17500: -276.36\n",
      "Training losses at iter 18000: -281.37\n",
      "Training losses at iter 18500: -277.3\n",
      "Training losses at iter 19000: -281.62\n",
      "Training losses at iter 19500: -286.12\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -277.96\n",
      "Training losses at iter 20500: -286.63\n",
      "Training losses at iter 21000: -285.96\n",
      "Training losses at iter 21500: -286.41\n",
      "Training losses at iter 22000: -288.82\n",
      "Training losses at iter 22500: -293.21\n",
      "Training losses at iter 23000: -289.01\n",
      "Training losses at iter 23500: -294.37\n",
      "Training losses at iter 24000: -291.52\n",
      "Training losses at iter 24500: -290.78\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -290.87\n",
      "Training losses at iter 25500: -292.3\n",
      "Training losses at iter 26000: -294.82\n",
      "Training losses at iter 26500: -292.23\n",
      "Training losses at iter 27000: -294.28\n",
      "Training losses at iter 27500: -293.66\n",
      "Training losses at iter 28000: -294.86\n",
      "Training losses at iter 28500: -292.94\n",
      "Training losses at iter 29000: -294.0\n",
      "Training losses at iter 29500: -300.18\n"
     ]
    }
   ],
   "source": [
    "# train P-GNN\n",
    "# urdf = urdf_to_train\n",
    "if RUN_SINGLE_GNN:\n",
    "    for urdf in urdf_names:\n",
    "        for condition_tuple in condition_tuples:\n",
    "\n",
    "            n_rollouts_to_use = condition_tuple[0]\n",
    "            seq_len = condition_tuple[1]\n",
    "\n",
    "            comment_str = '_pgnn_' + urdf + str(n_rollouts_to_use) + '_' + str(seq_len)\n",
    "            writer = SummaryWriter(log_dir = os.path.join(folder,'runs',\n",
    "                        start_time_str+ '_' + str(n_rollouts_to_use) + '_' + str(seq_len)),\n",
    "                        comment=comment_str)\n",
    "\n",
    "\n",
    "            # depending on the length of the multistep sequence we want,\n",
    "            # only some indexes of the full set of states collected can be sampled.\n",
    "            sampleable_inds = dict()\n",
    "            batch_sizes = dict()\n",
    "        #     for urdf in urdf_names:\n",
    "            sampleable_inds[urdf] = get_sampleable_inds(run_lens[urdf][:n_rollouts_to_use], seq_len)\n",
    "            n_sampleable = len(sampleable_inds[urdf])\n",
    "            batch_sizes[urdf] = batch_size_default\n",
    "            if batch_sizes[urdf] > n_sampleable:\n",
    "                batch_sizes[urdf] = n_sampleable\n",
    "            print(urdf + ' using ' + str(n_rollouts_to_use) + ' out of Rollouts ' + str(len(run_lens[urdf])))\n",
    "\n",
    "\n",
    "            batch_size = batch_sizes[urdf]\n",
    "            n_modules = len(modules_types[urdf])\n",
    "            module_state_len = module_sa_len[urdf][:n_modules]\n",
    "\n",
    "            # initialize network and optimizer\n",
    "            internal_state_len = 100\n",
    "            message_len = 50\n",
    "            hidden_layer_size = 250\n",
    "            weight_decay = 1e-4\n",
    "            gnn_nodes = pgnn.create_GNN_nodes(internal_state_len, message_len, hidden_layer_size, \n",
    "                            device, body_input = True)\n",
    "            optimizer = torch.optim.Adam(pgnn.get_GNN_params_list(gnn_nodes), \n",
    "                                         lr=1e-3,\n",
    "                                weight_decay= weight_decay)# create module containers for the nodes\n",
    "            modules = []\n",
    "            for i in range(n_modules):\n",
    "                modules.append(pgnn.Module(i, gnn_nodes[modules_types[urdf][i]], device))\n",
    "\n",
    "\n",
    "            num_nn_params=0\n",
    "            for p in pgnn.get_GNN_params_list(gnn_nodes):\n",
    "                nn=1\n",
    "                for s in list(p.size()):\n",
    "                    nn = nn*s\n",
    "                num_nn_params += nn\n",
    "\n",
    "            print('Num NN params: ' + str(num_nn_params))\n",
    "\n",
    "\n",
    "            for training_step in range(n_training_steps):\n",
    "        #     for training_step in range(0):\n",
    "\n",
    "                if np.mod(training_step,5000 )==0 and training_step>10000:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = param_group['lr']/2\n",
    "                        print( 'LR: ' + str(param_group['lr']) )\n",
    "\n",
    "                # sample without replacement from the full memory, depending on what is sampleable\n",
    "                state_seq, action_seq, sampled_inds = sample_memory(\n",
    "                                states_memory_tensors[urdf], actions_memory_tensors[urdf],\n",
    "                                sampleable_inds[urdf], seq_len, batch_size)\n",
    "\n",
    "                loss = 0\n",
    "                state_approx = to_device(state_seq[0],device) # initial state input is the first in sequence\n",
    "                for seq in range(seq_len-1): # for multistep loss, go through the sequence\n",
    "\n",
    "                    for module in modules: # must reset module lstm state\n",
    "                        module.reset_hidden_states(batch_size) \n",
    "\n",
    "                    # process states to move them to vehicle frame\n",
    "                    fd_input_real, delta_fd_real = to_body_frame_batch(state_seq[seq], state_seq[seq+1])\n",
    "                    fd_input_approx, R_t = state_to_fd_input(state_approx) # for recursive estimation\n",
    "\n",
    "                    # pass through network\n",
    "                    fd_input   = to_device(fd_input_approx, device) \n",
    "                    actions_in = to_device(action_seq[seq], device)\n",
    "                    delta_fd   = to_device(delta_fd_real, device) \n",
    "                    node_inputs = [torch.cat([s,a],1) for (s,a) in zip(fd_input, actions_in)]\n",
    "                    state_delta_est_mean, state_delta_var = pgnn.run_propagations(\n",
    "                        modules, attachments[urdf], 2, node_inputs, device)\n",
    "\n",
    "                    # compute loss for this step in sequence\n",
    "                    for mm in range(len(state_delta_est_mean)):\n",
    "                        loss += torch.sum(\n",
    "                            (state_delta_est_mean[mm] - delta_fd[mm])**2/state_delta_var[mm] + \n",
    "                            torch.log(state_delta_var[mm]) \n",
    "                                        )/batch_size/(seq_len-1)\n",
    "\n",
    "                    # transform back to world frame advance to next sequence step\n",
    "                    if seq_len>2:\n",
    "                        # update recursive state estimation for multistep loss\n",
    "                        # GNN output is already divided up into modules\n",
    "                        delta_fd_approx = state_delta_est_mean\n",
    "                        state_approx = from_body_frame_batch(state_approx, delta_fd_approx)\n",
    "\n",
    "\n",
    "\n",
    "                # backprop and optimizer step \n",
    "                loss_np=(loss.detach().cpu().numpy())\n",
    "                pgnn.zero_grads(gnn_nodes)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                writer.add_scalar('Train' + '/Loss_pgnn', loss_np, training_step)\n",
    "\n",
    "\n",
    "                # periodically save the model\n",
    "                if np.mod(training_step,500)==0:\n",
    "                    PATH = ('learned_models/' + urdf + '_pgnn_r' + str(int(n_rollouts_to_use)) + \n",
    "                            '_ms'+ str(int(seq_len))+'.pt')\n",
    "                    PATH = os.path.join(cwd, PATH)\n",
    "                    gnn_state_dicts=(pgnn.get_state_dicts(gnn_nodes))\n",
    "                    save_dict = dict()\n",
    "                    save_dict['gnn_state_dicts'] =  gnn_state_dicts\n",
    "                    save_dict['internal_state_len'] = gnn_nodes[0].internal_state_len\n",
    "                    save_dict['message_len'] = gnn_nodes[0].message_len\n",
    "                    save_dict['hidden_layer_size'] = gnn_nodes[0].hidden_layer_size\n",
    "                    save_dict['n_rollouts_to_use']=n_rollouts_to_use\n",
    "                    save_dict['seq_len']=seq_len\n",
    "                    save_dict['batch_size']=batch_size\n",
    "                    save_dict['urdf']=urdf\n",
    "                    save_dict['weight_decay'] = weight_decay\n",
    "                    save_dict['num_nn_params'] = num_nn_params\n",
    "\n",
    "                    torch.save(save_dict,  PATH)\n",
    "\n",
    "                    print('Training losses at iter ' + \n",
    "                        str(training_step) + ': ' + \n",
    "                        str(np.round(loss_np,2)))\n",
    "\n",
    "            del fd_input, actions_in, delta_fd,  loss, optimizer\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wnwwnw using 100 out of Rollouts 5000\n",
      "llllll using 100 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 18.63\n",
      "Training losses at iter 100: -185.51\n",
      "Training losses at iter 200: -194.93\n",
      "Training losses at iter 300: -221.23\n",
      "Training losses at iter 400: -223.74\n",
      "Training losses at iter 500: -227.19\n",
      "Training losses at iter 600: -241.83\n",
      "Training losses at iter 700: -250.51\n",
      "Training losses at iter 800: -260.44\n",
      "Training losses at iter 900: -268.68\n",
      "Training losses at iter 1000: -280.47\n",
      "Training losses at iter 1100: -278.79\n",
      "Training losses at iter 1200: -285.35\n",
      "Training losses at iter 1300: -294.93\n",
      "Training losses at iter 1400: -280.17\n",
      "Training losses at iter 1500: -301.6\n",
      "Training losses at iter 1600: -305.38\n",
      "Training losses at iter 1700: -315.69\n",
      "Training losses at iter 1800: -303.07\n",
      "Training losses at iter 1900: -315.52\n",
      "Training losses at iter 2000: -315.9\n",
      "Training losses at iter 2100: -316.38\n",
      "Training losses at iter 2200: -329.43\n",
      "Training losses at iter 2300: -329.93\n",
      "Training losses at iter 2400: -336.43\n",
      "Training losses at iter 2500: -328.0\n",
      "Training losses at iter 2600: -344.97\n",
      "Training losses at iter 2700: -331.38\n",
      "Training losses at iter 2800: -339.79\n",
      "Training losses at iter 2900: -349.46\n",
      "Training losses at iter 3000: -347.4\n",
      "Training losses at iter 3100: -358.71\n",
      "Training losses at iter 3200: -351.25\n",
      "Training losses at iter 3300: -364.7\n",
      "Training losses at iter 3400: -365.97\n",
      "Training losses at iter 3500: -355.36\n",
      "Training losses at iter 3600: -366.88\n",
      "Training losses at iter 3700: -379.31\n",
      "Training losses at iter 3800: -369.35\n",
      "Training losses at iter 3900: -375.9\n",
      "Training losses at iter 4000: -331.07\n",
      "Training losses at iter 4100: -378.98\n",
      "Training losses at iter 4200: -363.88\n",
      "Training losses at iter 4300: -372.86\n",
      "Training losses at iter 4400: -381.64\n",
      "Training losses at iter 4500: -379.74\n",
      "Training losses at iter 4600: -390.59\n",
      "Training losses at iter 4700: -394.2\n",
      "Training losses at iter 4800: -390.77\n",
      "Training losses at iter 4900: -388.98\n",
      "Training losses at iter 5000: -396.08\n",
      "Training losses at iter 5100: -403.7\n",
      "Training losses at iter 5200: -394.19\n",
      "Training losses at iter 5300: -404.84\n",
      "Training losses at iter 5400: -392.87\n",
      "Training losses at iter 5500: -351.2\n",
      "Training losses at iter 5600: -397.51\n",
      "Training losses at iter 5700: -414.4\n",
      "Training losses at iter 5800: -407.49\n",
      "Training losses at iter 5900: -405.93\n",
      "Training losses at iter 6000: -397.26\n",
      "Training losses at iter 6100: -419.12\n",
      "Training losses at iter 6200: -418.2\n",
      "Training losses at iter 6300: -422.21\n",
      "Training losses at iter 6400: -407.91\n",
      "Training losses at iter 6500: -416.41\n",
      "Training losses at iter 6600: -408.62\n",
      "Training losses at iter 6700: -428.83\n",
      "Training losses at iter 6800: -419.11\n",
      "Training losses at iter 6900: -438.91\n",
      "Training losses at iter 7000: -415.78\n",
      "Training losses at iter 7100: -432.13\n",
      "Training losses at iter 7200: -439.13\n",
      "Training losses at iter 7300: -426.09\n",
      "Training losses at iter 7400: -428.45\n",
      "Training losses at iter 7500: -421.55\n",
      "Training losses at iter 7600: -400.76\n",
      "Training losses at iter 7700: -429.04\n",
      "Training losses at iter 7800: -439.68\n",
      "Training losses at iter 7900: -442.94\n",
      "Training losses at iter 8000: -447.96\n",
      "Training losses at iter 8100: -447.67\n",
      "Training losses at iter 8200: -444.52\n",
      "Training losses at iter 8300: -404.83\n",
      "Training losses at iter 8400: -436.86\n",
      "Training losses at iter 8500: -437.99\n",
      "Training losses at iter 8600: -445.5\n",
      "Training losses at iter 8700: -426.01\n",
      "Training losses at iter 8800: -449.01\n",
      "Training losses at iter 8900: -429.63\n",
      "Training losses at iter 9000: -460.49\n",
      "Training losses at iter 9100: -436.69\n",
      "Training losses at iter 9200: -460.97\n",
      "Training losses at iter 9300: -435.11\n",
      "Training losses at iter 9400: -454.27\n",
      "Training losses at iter 9500: -442.7\n",
      "Training losses at iter 9600: -470.42\n",
      "Training losses at iter 9700: -465.52\n",
      "Training losses at iter 9800: -453.94\n",
      "Training losses at iter 9900: -452.14\n",
      "Training losses at iter 10000: -464.87\n",
      "Training losses at iter 10100: -439.49\n",
      "Training losses at iter 10200: -434.88\n",
      "Training losses at iter 10300: -458.87\n",
      "Training losses at iter 10400: -459.85\n",
      "Training losses at iter 10500: -477.75\n",
      "Training losses at iter 10600: -475.81\n",
      "Training losses at iter 10700: -459.75\n",
      "Training losses at iter 10800: -481.91\n",
      "Training losses at iter 10900: -476.95\n",
      "Training losses at iter 11000: -481.21\n",
      "Training losses at iter 11100: -468.69\n",
      "Training losses at iter 11200: -482.72\n",
      "Training losses at iter 11300: -483.36\n",
      "Training losses at iter 11400: -476.92\n",
      "Training losses at iter 11500: -486.24\n",
      "Training losses at iter 11600: -487.79\n",
      "Training losses at iter 11700: -475.16\n",
      "Training losses at iter 11800: -475.52\n",
      "Training losses at iter 11900: -481.93\n",
      "Training losses at iter 12000: -482.23\n",
      "Training losses at iter 12100: -481.59\n",
      "Training losses at iter 12200: -481.03\n",
      "Training losses at iter 12300: -494.7\n",
      "Training losses at iter 12400: -493.45\n",
      "Training losses at iter 12500: -489.37\n",
      "Training losses at iter 12600: -442.82\n",
      "Training losses at iter 12700: -489.96\n",
      "Training losses at iter 12800: -477.64\n",
      "Training losses at iter 12900: -489.24\n",
      "Training losses at iter 13000: -473.64\n",
      "Training losses at iter 13100: -475.9\n",
      "Training losses at iter 13200: -496.37\n",
      "Training losses at iter 13300: -488.79\n",
      "Training losses at iter 13400: -503.06\n",
      "Training losses at iter 13500: -492.83\n",
      "Training losses at iter 13600: -493.52\n",
      "Training losses at iter 13700: -505.35\n",
      "Training losses at iter 13800: -497.9\n",
      "Training losses at iter 13900: -500.93\n",
      "Training losses at iter 14000: -503.81\n",
      "Training losses at iter 14100: -510.5\n",
      "Training losses at iter 14200: -469.49\n",
      "Training losses at iter 14300: -500.63\n",
      "Training losses at iter 14400: -513.45\n",
      "Training losses at iter 14500: -500.31\n",
      "Training losses at iter 14600: -507.6\n",
      "Training losses at iter 14700: -372.04\n",
      "Training losses at iter 14800: -394.29\n",
      "Training losses at iter 14900: -450.32\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -481.12\n",
      "Training losses at iter 15100: -492.15\n",
      "Training losses at iter 15200: -502.07\n",
      "Training losses at iter 15300: -507.12\n",
      "Training losses at iter 15400: -510.72\n",
      "Training losses at iter 15500: -519.14\n",
      "Training losses at iter 15600: -515.49\n",
      "Training losses at iter 15700: -514.72\n",
      "Training losses at iter 15800: -516.39\n",
      "Training losses at iter 15900: -525.25\n",
      "Training losses at iter 16000: -523.14\n",
      "Training losses at iter 16100: -521.55\n",
      "Training losses at iter 16200: -526.3\n",
      "Training losses at iter 16300: -525.61\n",
      "Training losses at iter 16400: -532.33\n",
      "Training losses at iter 16500: -532.41\n",
      "Training losses at iter 16600: -527.84\n",
      "Training losses at iter 16700: -513.4\n",
      "Training losses at iter 16800: -529.86\n",
      "Training losses at iter 16900: -536.76\n",
      "Training losses at iter 17000: -532.52\n",
      "Training losses at iter 17100: -524.42\n",
      "Training losses at iter 17200: -532.4\n",
      "Training losses at iter 17300: -533.04\n",
      "Training losses at iter 17400: -540.04\n",
      "Training losses at iter 17500: -532.87\n",
      "Training losses at iter 17600: -536.09\n",
      "Training losses at iter 17700: -535.34\n",
      "Training losses at iter 17800: -543.25\n",
      "Training losses at iter 17900: -537.94\n",
      "Training losses at iter 18000: -532.9\n",
      "Training losses at iter 18100: -539.12\n",
      "Training losses at iter 18200: -542.54\n",
      "Training losses at iter 18300: -541.99\n",
      "Training losses at iter 18400: -539.78\n",
      "Training losses at iter 18500: -540.54\n",
      "Training losses at iter 18600: -549.25\n",
      "Training losses at iter 18700: -539.89\n",
      "Training losses at iter 18800: -548.35\n",
      "Training losses at iter 18900: -545.17\n",
      "Training losses at iter 19000: -545.57\n",
      "Training losses at iter 19100: -544.21\n",
      "Training losses at iter 19200: -544.54\n",
      "Training losses at iter 19300: -545.73\n",
      "Training losses at iter 19400: -550.11\n",
      "Training losses at iter 19500: -546.44\n",
      "Training losses at iter 19600: -541.91\n",
      "Training losses at iter 19700: -554.54\n",
      "Training losses at iter 19800: -548.67\n",
      "Training losses at iter 19900: -551.46\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -548.89\n",
      "Training losses at iter 20100: -563.52\n",
      "Training losses at iter 20200: -562.4\n",
      "Training losses at iter 20300: -565.87\n",
      "Training losses at iter 20400: -558.57\n",
      "Training losses at iter 20500: -564.61\n",
      "Training losses at iter 20600: -564.34\n",
      "Training losses at iter 20700: -567.23\n",
      "Training losses at iter 20800: -566.53\n",
      "Training losses at iter 20900: -563.87\n",
      "Training losses at iter 21000: -566.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses at iter 21100: -566.18\n",
      "Training losses at iter 21200: -559.27\n",
      "Training losses at iter 21300: -565.07\n",
      "Training losses at iter 21400: -564.43\n",
      "Training losses at iter 21500: -570.99\n",
      "Training losses at iter 21600: -566.94\n",
      "Training losses at iter 21700: -569.2\n",
      "Training losses at iter 21800: -567.46\n",
      "Training losses at iter 21900: -570.51\n",
      "Training losses at iter 22000: -570.47\n",
      "Training losses at iter 22100: -575.49\n",
      "Training losses at iter 22200: -570.08\n",
      "Training losses at iter 22300: -572.35\n",
      "Training losses at iter 22400: -566.83\n",
      "Training losses at iter 22500: -568.51\n",
      "Training losses at iter 22600: -568.41\n",
      "Training losses at iter 22700: -568.89\n",
      "Training losses at iter 22800: -569.21\n",
      "Training losses at iter 22900: -574.94\n",
      "Training losses at iter 23000: -572.58\n",
      "Training losses at iter 23100: -572.22\n",
      "Training losses at iter 23200: -568.72\n",
      "Training losses at iter 23300: -565.45\n",
      "Training losses at iter 23400: -568.57\n",
      "Training losses at iter 23500: -572.1\n",
      "Training losses at iter 23600: -570.8\n",
      "Training losses at iter 23700: -568.65\n",
      "Training losses at iter 23800: -572.42\n",
      "Training losses at iter 23900: -571.45\n",
      "Training losses at iter 24000: -574.0\n",
      "Training losses at iter 24100: -578.59\n",
      "Training losses at iter 24200: -570.91\n",
      "Training losses at iter 24300: -574.21\n",
      "Training losses at iter 24400: -573.55\n",
      "Training losses at iter 24500: -569.63\n",
      "Training losses at iter 24600: -571.4\n",
      "Training losses at iter 24700: -569.09\n",
      "Training losses at iter 24800: -572.69\n",
      "Training losses at iter 24900: -571.91\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -574.62\n",
      "Training losses at iter 25100: -582.44\n",
      "Training losses at iter 25200: -581.57\n",
      "Training losses at iter 25300: -582.94\n",
      "Training losses at iter 25400: -588.61\n",
      "Training losses at iter 25500: -584.76\n",
      "Training losses at iter 25600: -586.12\n",
      "Training losses at iter 25700: -584.17\n",
      "Training losses at iter 25800: -584.5\n",
      "Training losses at iter 25900: -580.4\n",
      "Training losses at iter 26000: -586.95\n",
      "Training losses at iter 26100: -587.39\n",
      "Training losses at iter 26200: -588.88\n",
      "Training losses at iter 26300: -587.26\n",
      "Training losses at iter 26400: -583.27\n",
      "Training losses at iter 26500: -583.87\n",
      "Training losses at iter 26600: -586.52\n",
      "Training losses at iter 26700: -580.69\n",
      "Training losses at iter 26800: -588.18\n",
      "Training losses at iter 26900: -584.97\n",
      "Training losses at iter 27000: -588.2\n",
      "Training losses at iter 27100: -586.79\n",
      "Training losses at iter 27200: -580.56\n",
      "Training losses at iter 27300: -584.36\n",
      "Training losses at iter 27400: -583.04\n",
      "Training losses at iter 27500: -588.89\n",
      "Training losses at iter 27600: -587.52\n",
      "Training losses at iter 27700: -588.41\n",
      "Training losses at iter 27800: -586.67\n",
      "Training losses at iter 27900: -585.7\n",
      "Training losses at iter 28000: -587.24\n",
      "Training losses at iter 28100: -587.68\n",
      "Training losses at iter 28200: -586.03\n",
      "Training losses at iter 28300: -587.98\n",
      "Training losses at iter 28400: -595.53\n",
      "Training losses at iter 28500: -587.16\n",
      "Training losses at iter 28600: -587.14\n",
      "Training losses at iter 28700: -587.58\n",
      "Training losses at iter 28800: -584.43\n",
      "Training losses at iter 28900: -587.63\n",
      "Training losses at iter 29000: -590.05\n",
      "Training losses at iter 29100: -589.31\n",
      "Training losses at iter 29200: -589.13\n",
      "Training losses at iter 29300: -589.22\n",
      "Training losses at iter 29400: -593.02\n",
      "Training losses at iter 29500: -592.61\n",
      "Training losses at iter 29600: -590.57\n",
      "Training losses at iter 29700: -594.5\n",
      "Training losses at iter 29800: -595.7\n",
      "Training losses at iter 29900: -589.03\n",
      "wnwwnw using 1000 out of Rollouts 5000\n",
      "llllll using 1000 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 16.48\n",
      "Training losses at iter 100: -183.7\n",
      "Training losses at iter 200: -207.13\n",
      "Training losses at iter 300: -217.83\n",
      "Training losses at iter 400: -220.95\n",
      "Training losses at iter 500: -238.36\n",
      "Training losses at iter 600: -245.02\n",
      "Training losses at iter 700: -248.43\n",
      "Training losses at iter 800: -265.51\n",
      "Training losses at iter 900: -225.88\n",
      "Training losses at iter 1000: -274.89\n",
      "Training losses at iter 1100: -268.0\n",
      "Training losses at iter 1200: -266.89\n",
      "Training losses at iter 1300: -288.36\n",
      "Training losses at iter 1400: -237.25\n",
      "Training losses at iter 1500: -291.09\n",
      "Training losses at iter 1600: -296.73\n",
      "Training losses at iter 1700: -301.95\n",
      "Training losses at iter 1800: -299.14\n",
      "Training losses at iter 1900: -304.0\n",
      "Training losses at iter 2000: -291.04\n",
      "Training losses at iter 2100: -254.71\n",
      "Training losses at iter 2200: -311.16\n",
      "Training losses at iter 2300: -308.3\n",
      "Training losses at iter 2400: -314.53\n",
      "Training losses at iter 2500: -285.62\n",
      "Training losses at iter 2600: -307.66\n",
      "Training losses at iter 2700: -319.4\n",
      "Training losses at iter 2800: -321.92\n",
      "Training losses at iter 2900: -322.33\n",
      "Training losses at iter 3000: -322.88\n",
      "Training losses at iter 3100: -330.63\n",
      "Training losses at iter 3200: -322.82\n",
      "Training losses at iter 3300: -319.9\n",
      "Training losses at iter 3400: -329.87\n",
      "Training losses at iter 3500: -338.75\n",
      "Training losses at iter 3600: -337.93\n",
      "Training losses at iter 3700: -335.9\n",
      "Training losses at iter 3800: -326.15\n",
      "Training losses at iter 3900: -345.61\n",
      "Training losses at iter 4000: -344.08\n",
      "Training losses at iter 4100: -341.4\n",
      "Training losses at iter 4200: -345.22\n",
      "Training losses at iter 4300: -347.47\n",
      "Training losses at iter 4400: -350.82\n",
      "Training losses at iter 4500: -348.01\n",
      "Training losses at iter 4600: -358.15\n",
      "Training losses at iter 4700: -354.55\n",
      "Training losses at iter 4800: -346.17\n",
      "Training losses at iter 4900: -355.49\n",
      "Training losses at iter 5000: -356.33\n",
      "Training losses at iter 5100: -341.46\n",
      "Training losses at iter 5200: -359.21\n",
      "Training losses at iter 5300: -355.32\n",
      "Training losses at iter 5400: -361.34\n",
      "Training losses at iter 5500: -359.63\n",
      "Training losses at iter 5600: -370.38\n",
      "Training losses at iter 5700: -340.24\n",
      "Training losses at iter 5800: -362.99\n",
      "Training losses at iter 5900: -374.26\n",
      "Training losses at iter 6000: -368.94\n",
      "Training losses at iter 6100: -368.45\n",
      "Training losses at iter 6200: -371.9\n",
      "Training losses at iter 6300: -378.45\n",
      "Training losses at iter 6400: -373.08\n",
      "Training losses at iter 6500: -382.36\n",
      "Training losses at iter 6600: -379.24\n",
      "Training losses at iter 6700: -369.73\n",
      "Training losses at iter 6800: -380.65\n",
      "Training losses at iter 6900: -382.85\n",
      "Training losses at iter 7000: -373.95\n",
      "Training losses at iter 7100: -386.02\n",
      "Training losses at iter 7200: -381.8\n",
      "Training losses at iter 7300: -386.6\n",
      "Training losses at iter 7400: -379.05\n",
      "Training losses at iter 7500: -385.55\n",
      "Training losses at iter 7600: -393.14\n",
      "Training losses at iter 7700: -366.73\n",
      "Training losses at iter 7800: -388.48\n",
      "Training losses at iter 7900: -386.53\n",
      "Training losses at iter 8000: -380.51\n",
      "Training losses at iter 8100: -398.74\n",
      "Training losses at iter 8200: -397.77\n",
      "Training losses at iter 8300: -393.35\n",
      "Training losses at iter 8400: -401.83\n",
      "Training losses at iter 8500: -378.44\n",
      "Training losses at iter 8600: -393.32\n",
      "Training losses at iter 8700: -400.02\n",
      "Training losses at iter 8800: -397.3\n",
      "Training losses at iter 8900: -399.29\n",
      "Training losses at iter 9000: -401.25\n",
      "Training losses at iter 9100: -402.82\n",
      "Training losses at iter 9200: -397.68\n",
      "Training losses at iter 9300: -380.09\n",
      "Training losses at iter 9400: -408.57\n",
      "Training losses at iter 9500: -398.05\n",
      "Training losses at iter 9600: -408.91\n",
      "Training losses at iter 9700: -408.26\n",
      "Training losses at iter 9800: -414.63\n",
      "Training losses at iter 9900: -417.0\n",
      "Training losses at iter 10000: -405.98\n",
      "Training losses at iter 10100: -408.08\n",
      "Training losses at iter 10200: -408.72\n",
      "Training losses at iter 10300: -417.62\n",
      "Training losses at iter 10400: -403.17\n",
      "Training losses at iter 10500: -418.17\n",
      "Training losses at iter 10600: -400.62\n",
      "Training losses at iter 10700: -401.89\n",
      "Training losses at iter 10800: -408.74\n",
      "Training losses at iter 10900: -417.75\n",
      "Training losses at iter 11000: -419.89\n",
      "Training losses at iter 11100: -417.45\n",
      "Training losses at iter 11200: -420.09\n",
      "Training losses at iter 11300: -422.05\n",
      "Training losses at iter 11400: -411.17\n",
      "Training losses at iter 11500: -414.09\n",
      "Training losses at iter 11600: -423.88\n",
      "Training losses at iter 11700: -424.02\n",
      "Training losses at iter 11800: -420.63\n",
      "Training losses at iter 11900: -425.48\n",
      "Training losses at iter 12000: -418.25\n",
      "Training losses at iter 12100: -427.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses at iter 12200: -422.06\n",
      "Training losses at iter 12300: -428.98\n",
      "Training losses at iter 12400: -425.59\n",
      "Training losses at iter 12500: -429.32\n",
      "Training losses at iter 12600: -424.84\n",
      "Training losses at iter 12700: -426.15\n",
      "Training losses at iter 12800: -436.92\n",
      "Training losses at iter 12900: -433.96\n",
      "Training losses at iter 13000: -420.35\n",
      "Training losses at iter 13100: -415.9\n",
      "Training losses at iter 13200: -434.8\n",
      "Training losses at iter 13300: -426.61\n",
      "Training losses at iter 13400: -425.59\n",
      "Training losses at iter 13500: -430.57\n",
      "Training losses at iter 13600: -432.63\n",
      "Training losses at iter 13700: -426.77\n",
      "Training losses at iter 13800: -440.59\n",
      "Training losses at iter 13900: -433.16\n",
      "Training losses at iter 14000: -414.77\n",
      "Training losses at iter 14100: -441.77\n",
      "Training losses at iter 14200: -439.14\n",
      "Training losses at iter 14300: -425.4\n",
      "Training losses at iter 14400: -435.51\n",
      "Training losses at iter 14500: -443.66\n",
      "Training losses at iter 14600: -434.51\n",
      "Training losses at iter 14700: -430.8\n",
      "Training losses at iter 14800: -442.51\n",
      "Training losses at iter 14900: -439.1\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -427.84\n",
      "Training losses at iter 15100: -451.25\n",
      "Training losses at iter 15200: -455.56\n",
      "Training losses at iter 15300: -456.39\n",
      "Training losses at iter 15400: -454.73\n",
      "Training losses at iter 15500: -456.8\n",
      "Training losses at iter 15600: -458.21\n",
      "Training losses at iter 15700: -457.0\n",
      "Training losses at iter 15800: -456.63\n",
      "Training losses at iter 15900: -455.98\n",
      "Training losses at iter 16000: -457.72\n",
      "Training losses at iter 16100: -456.36\n",
      "Training losses at iter 16200: -456.44\n",
      "Training losses at iter 16300: -458.72\n",
      "Training losses at iter 16400: -460.25\n",
      "Training losses at iter 16500: -456.86\n",
      "Training losses at iter 16600: -464.03\n",
      "Training losses at iter 16700: -458.75\n",
      "Training losses at iter 16800: -458.51\n",
      "Training losses at iter 16900: -461.47\n",
      "Training losses at iter 17000: -463.6\n",
      "Training losses at iter 17100: -462.56\n",
      "Training losses at iter 17200: -460.52\n",
      "Training losses at iter 17300: -459.1\n",
      "Training losses at iter 17400: -462.78\n",
      "Training losses at iter 17500: -464.53\n",
      "Training losses at iter 17600: -463.06\n",
      "Training losses at iter 17700: -464.3\n",
      "Training losses at iter 17800: -463.3\n",
      "Training losses at iter 17900: -468.2\n",
      "Training losses at iter 18000: -465.9\n",
      "Training losses at iter 18100: -466.47\n",
      "Training losses at iter 18200: -464.87\n",
      "Training losses at iter 18300: -463.5\n",
      "Training losses at iter 18400: -462.35\n",
      "Training losses at iter 18500: -462.35\n",
      "Training losses at iter 18600: -468.88\n",
      "Training losses at iter 18700: -467.82\n",
      "Training losses at iter 18800: -466.32\n",
      "Training losses at iter 18900: -464.84\n",
      "Training losses at iter 19000: -466.84\n",
      "Training losses at iter 19100: -471.55\n",
      "Training losses at iter 19200: -467.47\n",
      "Training losses at iter 19300: -464.38\n",
      "Training losses at iter 19400: -465.43\n",
      "Training losses at iter 19500: -469.48\n",
      "Training losses at iter 19600: -472.9\n",
      "Training losses at iter 19700: -464.65\n",
      "Training losses at iter 19800: -469.22\n",
      "Training losses at iter 19900: -469.82\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -465.69\n",
      "Training losses at iter 20100: -477.77\n",
      "Training losses at iter 20200: -477.28\n",
      "Training losses at iter 20300: -480.54\n",
      "Training losses at iter 20400: -479.6\n",
      "Training losses at iter 20500: -482.52\n",
      "Training losses at iter 20600: -475.75\n",
      "Training losses at iter 20700: -479.66\n",
      "Training losses at iter 20800: -474.92\n",
      "Training losses at iter 20900: -480.21\n",
      "Training losses at iter 21000: -480.22\n",
      "Training losses at iter 21100: -475.72\n",
      "Training losses at iter 21200: -478.04\n",
      "Training losses at iter 21300: -477.82\n",
      "Training losses at iter 21400: -476.93\n",
      "Training losses at iter 21500: -480.42\n",
      "Training losses at iter 21600: -477.77\n",
      "Training losses at iter 21700: -478.3\n",
      "Training losses at iter 21800: -477.64\n",
      "Training losses at iter 21900: -479.06\n",
      "Training losses at iter 22000: -479.97\n",
      "Training losses at iter 22100: -480.0\n",
      "Training losses at iter 22200: -480.8\n",
      "Training losses at iter 22300: -481.56\n",
      "Training losses at iter 22400: -479.07\n",
      "Training losses at iter 22500: -478.86\n",
      "Training losses at iter 22600: -480.85\n",
      "Training losses at iter 22700: -481.94\n",
      "Training losses at iter 22800: -484.63\n",
      "Training losses at iter 22900: -479.97\n",
      "Training losses at iter 23000: -481.69\n",
      "Training losses at iter 23100: -485.16\n",
      "Training losses at iter 23200: -479.9\n",
      "Training losses at iter 23300: -479.5\n",
      "Training losses at iter 23400: -483.22\n",
      "Training losses at iter 23500: -482.27\n",
      "Training losses at iter 23600: -483.28\n",
      "Training losses at iter 23700: -488.24\n",
      "Training losses at iter 23800: -493.98\n",
      "Training losses at iter 23900: -482.42\n",
      "Training losses at iter 24000: -482.05\n",
      "Training losses at iter 24100: -483.49\n",
      "Training losses at iter 24200: -486.59\n",
      "Training losses at iter 24300: -483.65\n",
      "Training losses at iter 24400: -481.27\n",
      "Training losses at iter 24500: -483.91\n",
      "Training losses at iter 24600: -486.25\n",
      "Training losses at iter 24700: -486.43\n",
      "Training losses at iter 24800: -490.42\n",
      "Training losses at iter 24900: -489.25\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -485.82\n",
      "Training losses at iter 25100: -492.25\n",
      "Training losses at iter 25200: -492.34\n",
      "Training losses at iter 25300: -490.48\n",
      "Training losses at iter 25400: -490.55\n",
      "Training losses at iter 25500: -489.27\n",
      "Training losses at iter 25600: -487.46\n",
      "Training losses at iter 25700: -488.11\n",
      "Training losses at iter 25800: -479.57\n",
      "Training losses at iter 25900: -490.3\n",
      "Training losses at iter 26000: -489.77\n",
      "Training losses at iter 26100: -486.87\n",
      "Training losses at iter 26200: -493.75\n",
      "Training losses at iter 26300: -485.65\n",
      "Training losses at iter 26400: -492.27\n",
      "Training losses at iter 26500: -491.52\n",
      "Training losses at iter 26600: -484.62\n",
      "Training losses at iter 26700: -491.14\n",
      "Training losses at iter 26800: -489.72\n",
      "Training losses at iter 26900: -491.65\n",
      "Training losses at iter 27000: -488.92\n",
      "Training losses at iter 27100: -491.57\n",
      "Training losses at iter 27200: -489.35\n",
      "Training losses at iter 27300: -495.76\n",
      "Training losses at iter 27400: -492.06\n",
      "Training losses at iter 27500: -494.15\n",
      "Training losses at iter 27600: -494.11\n",
      "Training losses at iter 27700: -495.63\n",
      "Training losses at iter 27800: -492.62\n",
      "Training losses at iter 27900: -487.36\n",
      "Training losses at iter 28000: -489.36\n",
      "Training losses at iter 28100: -488.15\n",
      "Training losses at iter 28200: -486.58\n",
      "Training losses at iter 28300: -490.85\n",
      "Training losses at iter 28400: -492.91\n",
      "Training losses at iter 28500: -489.88\n",
      "Training losses at iter 28600: -490.36\n",
      "Training losses at iter 28700: -487.77\n",
      "Training losses at iter 28800: -491.48\n",
      "Training losses at iter 28900: -491.01\n",
      "Training losses at iter 29000: -495.11\n",
      "Training losses at iter 29100: -496.76\n",
      "Training losses at iter 29200: -488.66\n",
      "Training losses at iter 29300: -493.19\n",
      "Training losses at iter 29400: -493.55\n",
      "Training losses at iter 29500: -498.05\n",
      "Training losses at iter 29600: -496.23\n",
      "Training losses at iter 29700: -494.19\n",
      "Training losses at iter 29800: -489.56\n",
      "Training losses at iter 29900: -495.64\n",
      "wnwwnw using 5000 out of Rollouts 5000\n",
      "llllll using 5000 out of Rollouts 5000\n",
      "Num NN params: 495784\n",
      "Training losses at iter 0: 19.33\n",
      "Training losses at iter 100: -185.54\n",
      "Training losses at iter 200: -205.55\n",
      "Training losses at iter 300: -212.17\n",
      "Training losses at iter 400: -223.1\n",
      "Training losses at iter 500: -220.71\n",
      "Training losses at iter 600: -237.42\n",
      "Training losses at iter 700: -250.79\n",
      "Training losses at iter 800: -218.89\n",
      "Training losses at iter 900: -231.82\n",
      "Training losses at iter 1000: -249.44\n",
      "Training losses at iter 1100: -186.48\n",
      "Training losses at iter 1200: -262.61\n",
      "Training losses at iter 1300: -276.99\n",
      "Training losses at iter 1400: -276.47\n",
      "Training losses at iter 1500: -286.81\n",
      "Training losses at iter 1600: -285.34\n",
      "Training losses at iter 1700: -281.43\n",
      "Training losses at iter 1800: -276.12\n",
      "Training losses at iter 1900: -281.11\n",
      "Training losses at iter 2000: -285.94\n",
      "Training losses at iter 2100: -295.33\n",
      "Training losses at iter 2200: -296.72\n",
      "Training losses at iter 2300: -290.93\n",
      "Training losses at iter 2400: -304.56\n",
      "Training losses at iter 2500: -293.41\n",
      "Training losses at iter 2600: -303.2\n",
      "Training losses at iter 2700: -262.01\n",
      "Training losses at iter 2800: -297.37\n",
      "Training losses at iter 2900: -314.1\n",
      "Training losses at iter 3000: -310.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses at iter 3100: -309.25\n",
      "Training losses at iter 3200: -323.37\n",
      "Training losses at iter 3300: -324.0\n",
      "Training losses at iter 3400: -322.56\n",
      "Training losses at iter 3500: -314.78\n",
      "Training losses at iter 3600: -327.35\n",
      "Training losses at iter 3700: -329.76\n",
      "Training losses at iter 3800: -331.53\n",
      "Training losses at iter 3900: -319.99\n",
      "Training losses at iter 4000: -338.86\n",
      "Training losses at iter 4100: -344.43\n",
      "Training losses at iter 4200: -338.72\n",
      "Training losses at iter 4300: -341.79\n",
      "Training losses at iter 4400: -343.95\n",
      "Training losses at iter 4500: -347.8\n",
      "Training losses at iter 4600: -342.36\n",
      "Training losses at iter 4700: -348.34\n",
      "Training losses at iter 4800: -344.97\n",
      "Training losses at iter 4900: -337.05\n",
      "Training losses at iter 5000: -356.34\n",
      "Training losses at iter 5100: -350.93\n",
      "Training losses at iter 5200: -347.23\n",
      "Training losses at iter 5300: -339.57\n",
      "Training losses at iter 5400: -358.96\n",
      "Training losses at iter 5500: -351.05\n",
      "Training losses at iter 5600: -353.26\n",
      "Training losses at iter 5700: -362.31\n",
      "Training losses at iter 5800: -361.2\n",
      "Training losses at iter 5900: -367.13\n",
      "Training losses at iter 6000: -367.03\n",
      "Training losses at iter 6100: -358.58\n",
      "Training losses at iter 6200: -362.85\n",
      "Training losses at iter 6300: -360.31\n",
      "Training losses at iter 6400: -349.89\n",
      "Training losses at iter 6500: -368.75\n",
      "Training losses at iter 6600: -369.13\n",
      "Training losses at iter 6700: -375.59\n",
      "Training losses at iter 6800: -363.66\n",
      "Training losses at iter 6900: -372.35\n",
      "Training losses at iter 7000: -370.13\n",
      "Training losses at iter 7100: -374.58\n",
      "Training losses at iter 7200: -366.36\n",
      "Training losses at iter 7300: -368.03\n",
      "Training losses at iter 7400: -372.45\n",
      "Training losses at iter 7500: -370.19\n",
      "Training losses at iter 7600: -381.09\n",
      "Training losses at iter 7700: -377.5\n",
      "Training losses at iter 7800: -368.76\n",
      "Training losses at iter 7900: -375.15\n",
      "Training losses at iter 8000: -379.72\n",
      "Training losses at iter 8100: -381.14\n",
      "Training losses at iter 8200: -376.06\n",
      "Training losses at iter 8300: -378.35\n",
      "Training losses at iter 8400: -377.25\n",
      "Training losses at iter 8500: -376.39\n",
      "Training losses at iter 8600: -374.6\n",
      "Training losses at iter 8700: -380.48\n",
      "Training losses at iter 8800: -377.95\n",
      "Training losses at iter 8900: -386.18\n",
      "Training losses at iter 9000: -393.07\n",
      "Training losses at iter 9100: -372.72\n",
      "Training losses at iter 9200: -372.79\n",
      "Training losses at iter 9300: -377.37\n",
      "Training losses at iter 9400: -385.28\n",
      "Training losses at iter 9500: -386.55\n",
      "Training losses at iter 9600: -387.91\n",
      "Training losses at iter 9700: -383.35\n",
      "Training losses at iter 9800: -385.47\n",
      "Training losses at iter 9900: -386.29\n",
      "Training losses at iter 10000: -390.35\n",
      "Training losses at iter 10100: -377.72\n",
      "Training losses at iter 10200: -379.13\n",
      "Training losses at iter 10300: -388.42\n",
      "Training losses at iter 10400: -374.91\n",
      "Training losses at iter 10500: -384.5\n",
      "Training losses at iter 10600: -391.0\n",
      "Training losses at iter 10700: -393.87\n",
      "Training losses at iter 10800: -405.07\n",
      "Training losses at iter 10900: -393.24\n",
      "Training losses at iter 11000: -396.4\n",
      "Training losses at iter 11100: -396.44\n",
      "Training losses at iter 11200: -386.02\n",
      "Training losses at iter 11300: -397.56\n",
      "Training losses at iter 11400: -392.55\n",
      "Training losses at iter 11500: -375.17\n",
      "Training losses at iter 11600: -398.04\n",
      "Training losses at iter 11700: -395.03\n",
      "Training losses at iter 11800: -396.75\n",
      "Training losses at iter 11900: -397.37\n",
      "Training losses at iter 12000: -402.39\n",
      "Training losses at iter 12100: -397.78\n",
      "Training losses at iter 12200: -398.2\n",
      "Training losses at iter 12300: -413.5\n",
      "Training losses at iter 12400: -406.37\n",
      "Training losses at iter 12500: -399.4\n",
      "Training losses at iter 12600: -397.24\n",
      "Training losses at iter 12700: -402.77\n",
      "Training losses at iter 12800: -399.12\n",
      "Training losses at iter 12900: -392.88\n",
      "Training losses at iter 13000: -397.93\n",
      "Training losses at iter 13100: -407.22\n",
      "Training losses at iter 13200: -397.15\n",
      "Training losses at iter 13300: -402.52\n",
      "Training losses at iter 13400: -405.42\n",
      "Training losses at iter 13500: -407.78\n",
      "Training losses at iter 13600: -406.39\n",
      "Training losses at iter 13700: -398.11\n",
      "Training losses at iter 13800: -408.37\n",
      "Training losses at iter 13900: -403.84\n",
      "Training losses at iter 14000: -414.98\n",
      "Training losses at iter 14100: -403.87\n",
      "Training losses at iter 14200: -406.83\n",
      "Training losses at iter 14300: -405.33\n",
      "Training losses at iter 14400: -411.43\n",
      "Training losses at iter 14500: -411.45\n",
      "Training losses at iter 14600: -405.66\n",
      "Training losses at iter 14700: -416.38\n",
      "Training losses at iter 14800: -417.99\n",
      "Training losses at iter 14900: -410.62\n",
      "LR: 0.0005\n",
      "Training losses at iter 15000: -420.09\n",
      "Training losses at iter 15100: -421.18\n",
      "Training losses at iter 15200: -426.61\n",
      "Training losses at iter 15300: -421.19\n",
      "Training losses at iter 15400: -425.77\n",
      "Training losses at iter 15500: -422.56\n",
      "Training losses at iter 15600: -425.57\n",
      "Training losses at iter 15700: -422.48\n",
      "Training losses at iter 15800: -425.62\n",
      "Training losses at iter 15900: -431.84\n",
      "Training losses at iter 16000: -423.97\n",
      "Training losses at iter 16100: -427.89\n",
      "Training losses at iter 16200: -426.28\n",
      "Training losses at iter 16300: -429.97\n",
      "Training losses at iter 16400: -428.08\n",
      "Training losses at iter 16500: -424.13\n",
      "Training losses at iter 16600: -432.41\n",
      "Training losses at iter 16700: -429.85\n",
      "Training losses at iter 16800: -427.17\n",
      "Training losses at iter 16900: -435.02\n",
      "Training losses at iter 17000: -437.13\n",
      "Training losses at iter 17100: -436.07\n",
      "Training losses at iter 17200: -433.51\n",
      "Training losses at iter 17300: -434.8\n",
      "Training losses at iter 17400: -429.32\n",
      "Training losses at iter 17500: -431.58\n",
      "Training losses at iter 17600: -433.74\n",
      "Training losses at iter 17700: -434.86\n",
      "Training losses at iter 17800: -435.98\n",
      "Training losses at iter 17900: -422.97\n",
      "Training losses at iter 18000: -430.67\n",
      "Training losses at iter 18100: -436.23\n",
      "Training losses at iter 18200: -427.8\n",
      "Training losses at iter 18300: -432.08\n",
      "Training losses at iter 18400: -437.9\n",
      "Training losses at iter 18500: -439.19\n",
      "Training losses at iter 18600: -439.03\n",
      "Training losses at iter 18700: -443.62\n",
      "Training losses at iter 18800: -431.67\n",
      "Training losses at iter 18900: -433.7\n",
      "Training losses at iter 19000: -436.69\n",
      "Training losses at iter 19100: -437.44\n",
      "Training losses at iter 19200: -438.95\n",
      "Training losses at iter 19300: -437.91\n",
      "Training losses at iter 19400: -440.49\n",
      "Training losses at iter 19500: -426.79\n",
      "Training losses at iter 19600: -442.91\n",
      "Training losses at iter 19700: -432.36\n",
      "Training losses at iter 19800: -435.86\n",
      "Training losses at iter 19900: -439.9\n",
      "LR: 0.00025\n",
      "Training losses at iter 20000: -437.23\n",
      "Training losses at iter 20100: -441.19\n",
      "Training losses at iter 20200: -445.73\n",
      "Training losses at iter 20300: -453.88\n",
      "Training losses at iter 20400: -446.08\n",
      "Training losses at iter 20500: -439.62\n",
      "Training losses at iter 20600: -447.33\n",
      "Training losses at iter 20700: -440.74\n",
      "Training losses at iter 20800: -447.76\n",
      "Training losses at iter 20900: -434.38\n",
      "Training losses at iter 21000: -445.15\n",
      "Training losses at iter 21100: -451.5\n",
      "Training losses at iter 21200: -446.11\n",
      "Training losses at iter 21300: -448.74\n",
      "Training losses at iter 21400: -443.99\n",
      "Training losses at iter 21500: -444.92\n",
      "Training losses at iter 21600: -443.43\n",
      "Training losses at iter 21700: -447.14\n",
      "Training losses at iter 21800: -448.78\n",
      "Training losses at iter 21900: -448.99\n",
      "Training losses at iter 22000: -445.01\n",
      "Training losses at iter 22100: -447.84\n",
      "Training losses at iter 22200: -447.81\n",
      "Training losses at iter 22300: -442.95\n",
      "Training losses at iter 22400: -444.57\n",
      "Training losses at iter 22500: -448.37\n",
      "Training losses at iter 22600: -445.21\n",
      "Training losses at iter 22700: -441.43\n",
      "Training losses at iter 22800: -450.61\n",
      "Training losses at iter 22900: -443.18\n",
      "Training losses at iter 23000: -450.94\n",
      "Training losses at iter 23100: -447.8\n",
      "Training losses at iter 23200: -452.0\n",
      "Training losses at iter 23300: -447.01\n",
      "Training losses at iter 23400: -447.25\n",
      "Training losses at iter 23500: -446.32\n",
      "Training losses at iter 23600: -452.6\n",
      "Training losses at iter 23700: -448.56\n",
      "Training losses at iter 23800: -449.18\n",
      "Training losses at iter 23900: -452.28\n",
      "Training losses at iter 24000: -453.51\n",
      "Training losses at iter 24100: -449.26\n",
      "Training losses at iter 24200: -454.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training losses at iter 24300: -451.16\n",
      "Training losses at iter 24400: -450.4\n",
      "Training losses at iter 24500: -453.31\n",
      "Training losses at iter 24600: -451.39\n",
      "Training losses at iter 24700: -457.31\n",
      "Training losses at iter 24800: -447.51\n",
      "Training losses at iter 24900: -454.46\n",
      "LR: 0.000125\n",
      "Training losses at iter 25000: -453.72\n",
      "Training losses at iter 25100: -451.72\n",
      "Training losses at iter 25200: -459.18\n",
      "Training losses at iter 25300: -456.1\n",
      "Training losses at iter 25400: -456.64\n",
      "Training losses at iter 25500: -456.99\n",
      "Training losses at iter 25600: -451.8\n",
      "Training losses at iter 25700: -455.15\n",
      "Training losses at iter 25800: -458.7\n",
      "Training losses at iter 25900: -455.22\n",
      "Training losses at iter 26000: -457.56\n",
      "Training losses at iter 26100: -455.81\n",
      "Training losses at iter 26200: -455.47\n",
      "Training losses at iter 26300: -461.21\n",
      "Training losses at iter 26400: -457.26\n",
      "Training losses at iter 26500: -453.86\n",
      "Training losses at iter 26600: -455.76\n",
      "Training losses at iter 26700: -457.78\n",
      "Training losses at iter 26800: -454.24\n",
      "Training losses at iter 26900: -452.64\n",
      "Training losses at iter 27000: -458.38\n",
      "Training losses at iter 27100: -452.83\n",
      "Training losses at iter 27200: -462.51\n",
      "Training losses at iter 27300: -455.74\n",
      "Training losses at iter 27400: -451.61\n",
      "Training losses at iter 27500: -459.88\n",
      "Training losses at iter 27600: -452.54\n",
      "Training losses at iter 27700: -456.92\n",
      "Training losses at iter 27800: -459.5\n",
      "Training losses at iter 27900: -459.25\n",
      "Training losses at iter 28000: -457.38\n",
      "Training losses at iter 28100: -452.44\n",
      "Training losses at iter 28200: -455.4\n",
      "Training losses at iter 28300: -458.74\n",
      "Training losses at iter 28400: -454.1\n",
      "Training losses at iter 28500: -464.83\n",
      "Training losses at iter 28600: -458.81\n",
      "Training losses at iter 28700: -457.45\n",
      "Training losses at iter 28800: -461.92\n",
      "Training losses at iter 28900: -458.97\n",
      "Training losses at iter 29000: -462.36\n",
      "Training losses at iter 29100: -455.66\n",
      "Training losses at iter 29200: -456.14\n",
      "Training losses at iter 29300: -464.35\n",
      "Training losses at iter 29400: -461.82\n",
      "Training losses at iter 29500: -461.15\n",
      "Training losses at iter 29600: -464.17\n",
      "Training losses at iter 29700: -465.11\n",
      "Training losses at iter 29800: -459.25\n",
      "Training losses at iter 29900: -456.48\n"
     ]
    }
   ],
   "source": [
    "if RUN_MULTI_GNN:\n",
    "    # train P-GNN, with multiple designs. this will be N_des times slower\n",
    "    for condition_tuple in condition_tuples:\n",
    "        n_rollouts_to_use = condition_tuple[0]\n",
    "        seq_len = condition_tuple[1]\n",
    "\n",
    "\n",
    "        comment_str = '_pgnn_multi_' + str(n_rollouts_to_use) + '_' + str(seq_len)\n",
    "        writer = SummaryWriter(log_dir = os.path.join(folder,'runs',\n",
    "                    start_time_str+ '_' + str(n_rollouts_to_use) + '_' + str(seq_len)),\n",
    "                    comment=comment_str)\n",
    "\n",
    "\n",
    "        # depending on the length of the multistep sequence we want,\n",
    "        # only some indexes of the full set of states collected can be sampled.\n",
    "        sampleable_inds = dict()\n",
    "        batch_sizes = dict()\n",
    "        for urdf in urdf_names:\n",
    "            sampleable_inds[urdf] = get_sampleable_inds(run_lens[urdf][:n_rollouts_to_use], seq_len)\n",
    "            n_sampleable = len(sampleable_inds[urdf])\n",
    "            batch_sizes[urdf] = batch_size_default\n",
    "            if batch_sizes[urdf] > n_sampleable:\n",
    "                batch_sizes[urdf] = n_sampleable\n",
    "            print(urdf + ' using ' + str(n_rollouts_to_use) + ' out of Rollouts ' + str(len(run_lens[urdf])))\n",
    "\n",
    "\n",
    "        # initialize network and optimizer\n",
    "        internal_state_len = 100\n",
    "        message_len = 50\n",
    "        hidden_layer_size = 250\n",
    "        weight_decay = 1e-4\n",
    "        gnn_nodes = pgnn.create_GNN_nodes(internal_state_len, message_len, hidden_layer_size, \n",
    "                        device, body_input = True)\n",
    "        optimizer = torch.optim.Adam(pgnn.get_GNN_params_list(gnn_nodes), \n",
    "                                     lr=1e-3,\n",
    "                            weight_decay= weight_decay)# create module containers for the nodes\n",
    "\n",
    "\n",
    "        modules = dict()\n",
    "        for urdf in urdf_names:\n",
    "            modules[urdf] = []\n",
    "            n_modules = len(modules_types[urdf])\n",
    "            for i in range(n_modules):\n",
    "                modules[urdf].append(pgnn.Module(i, gnn_nodes[modules_types[urdf][i]], device))\n",
    "\n",
    "        num_nn_params=0\n",
    "        for p in pgnn.get_GNN_params_list(gnn_nodes):\n",
    "            nn=1\n",
    "            for s in list(p.size()):\n",
    "                nn = nn*s\n",
    "            num_nn_params += nn\n",
    "        print('Num NN params: ' + str(num_nn_params))\n",
    "\n",
    "\n",
    "        for training_step in range(n_training_steps):\n",
    "\n",
    "            if np.mod(training_step,5000 )==0 and training_step>10000:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = param_group['lr']/2\n",
    "                    print( 'LR: ' + str(param_group['lr']) )\n",
    "\n",
    "            # accumulate gradients accros designs but not loss\n",
    "            optimizer.zero_grad()\n",
    "            loss_tot_np = 0\n",
    "            for urdf in urdf_names:\n",
    "                batch_size = batch_sizes[urdf]\n",
    "\n",
    "                # sample without replacement from the full memory, depending on what is sampleable\n",
    "                state_seq, action_seq, sampled_inds = sample_memory(\n",
    "                                states_memory_tensors[urdf], actions_memory_tensors[urdf],\n",
    "                                sampleable_inds[urdf], seq_len, batch_size)\n",
    "\n",
    "                loss = 0 # accumulate loss for a single design accross the multistep sequence\n",
    "                state_approx = to_device(state_seq[0],device) # initial state input is the first in sequence\n",
    "                for seq in range(seq_len-1): # for multistep loss, go through the sequence\n",
    "\n",
    "                    for module in modules[urdf]: # must reset module lstm state\n",
    "                        module.reset_hidden_states(batch_size) \n",
    "\n",
    "                    # process states to move them to vehicle frame\n",
    "                    fd_input_real, delta_fd_real = to_body_frame_batch(state_seq[seq], state_seq[seq+1])\n",
    "                    fd_input_approx, R_t = state_to_fd_input(state_approx) # for recursive estimation\n",
    "\n",
    "                    # pass through network\n",
    "                    fd_input   = to_device(fd_input_approx, device) \n",
    "                    actions_in = to_device(action_seq[seq], device)\n",
    "                    delta_fd   = to_device(delta_fd_real, device) \n",
    "                    node_inputs = [torch.cat([s,a],1) for (s,a) in zip(fd_input, actions_in)]\n",
    "                    state_delta_est_mean, state_delta_var = pgnn.run_propagations(\n",
    "                        modules[urdf], attachments[urdf], 2, node_inputs, device)\n",
    "\n",
    "                    # compute loss for this step in sequence\n",
    "                    for mm in range(len(state_delta_est_mean)):\n",
    "                        loss += torch.sum(\n",
    "                            (state_delta_est_mean[mm] - delta_fd[mm])**2/state_delta_var[mm] + \n",
    "                            torch.log(state_delta_var[mm]) \n",
    "                                        )/batch_size/(seq_len-1)\n",
    "\n",
    "                    # transform back to world frame advance to next sequence step\n",
    "                    if seq_len>2:\n",
    "                        # update recursive state estimation for multistep loss\n",
    "                        # GNN output is already divided up into modules\n",
    "                        delta_fd_approx = state_delta_est_mean\n",
    "                        state_approx = from_body_frame_batch(state_approx, delta_fd_approx)\n",
    "\n",
    "                # after multistep sequence, add loss for this design onto full loss for tracking\n",
    "                loss_np=(loss.detach().cpu().numpy())\n",
    "                loss_tot_np += loss_np\n",
    "\n",
    "                # backward for each design to keep compute tree smaller\n",
    "                loss.backward()\n",
    "\n",
    "            # optimizer step once we have accumulated grads for all designs\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('Train' + '/Loss_pgnn_multidesign', loss_tot_np, training_step)\n",
    "\n",
    "\n",
    "            # periodically save the model\n",
    "            if np.mod(training_step,100)==0:\n",
    "                PATH = ('learned_models/' + 'multidesign_pgnn_r' + str(int(n_rollouts_to_use)) + \n",
    "                        '_ms'+ str(int(seq_len))+'.pt')\n",
    "                PATH = os.path.join(cwd, PATH)\n",
    "                gnn_state_dicts=(pgnn.get_state_dicts(gnn_nodes))\n",
    "                save_dict = dict()\n",
    "                save_dict['gnn_state_dicts'] =  gnn_state_dicts\n",
    "                save_dict['internal_state_len'] = gnn_nodes[0].internal_state_len\n",
    "                save_dict['message_len'] = gnn_nodes[0].message_len\n",
    "                save_dict['hidden_layer_size'] = gnn_nodes[0].hidden_layer_size\n",
    "                save_dict['n_rollouts_to_use']=n_rollouts_to_use\n",
    "                save_dict['seq_len']=seq_len\n",
    "                save_dict['batch_sizes']=batch_sizes\n",
    "                save_dict['urdf_names']=urdf_names\n",
    "                save_dict['weight_decay'] = weight_decay\n",
    "                save_dict['num_nn_params'] = num_nn_params\n",
    "                torch.save(save_dict,  PATH)\n",
    "\n",
    "                print('Training losses at iter ' + \n",
    "                    str(training_step) + ': ' + \n",
    "                    str(np.round(loss_tot_np,2)))\n",
    "\n",
    "        del fd_input, actions_in, delta_fd\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found files \n",
      "['random_rollouts/wnwwnw_random_rollouts_validation.ptx']\n",
      "loading random_rollouts/wnwwnw_random_rollouts_validation.ptx\n",
      "Found files \n",
      "['random_rollouts/llllll_random_rollouts_validation.ptx']\n",
      "loading random_rollouts/llllll_random_rollouts_validation.ptx\n",
      "Found files \n",
      "['random_rollouts/llwwll_random_rollouts_validation.ptx']\n",
      "loading random_rollouts/llwwll_random_rollouts_validation.ptx\n",
      "loaded and merged data\n"
     ]
    }
   ],
   "source": [
    "# evaluation: load validation data set\n",
    "states_memory_validation =dict()\n",
    "actions_memory_validation =dict()\n",
    "run_lens_validation =dict()\n",
    "for urdf in urdf_names:\n",
    "    \n",
    "    file_names = []\n",
    "    folder = os.path.join(cwd, 'random_rollouts')\n",
    "    found = True\n",
    "    fname_test = os.path.join(folder,urdf+'_random_rollouts_validation.ptx')\n",
    "    if os.path.isfile(fname_test):\n",
    "        file_names.append(fname_test)\n",
    "    print('Found files ')\n",
    "    print(str(file_names))\n",
    "    \n",
    "    states_memory_validation[urdf] = []\n",
    "    actions_memory_validation[urdf] = []\n",
    "    run_lens_validation[urdf] = []\n",
    "\n",
    "    for fname in file_names:\n",
    "        print('loading ' + fname )\n",
    "        data_in = torch.load(fname)\n",
    "        states_memory_validation[urdf] += data_in['states_memory']\n",
    "        actions_memory_validation[urdf] += data_in['actions_memory']\n",
    "        run_lens_validation[urdf] += data_in['run_lens']\n",
    "        del data_in\n",
    "\n",
    "#     states_memory_tensors[urdf] = [torch.cat(s,0) for s in list(zip(*states_memory)) ]\n",
    "#     actions_memory_tensors[urdf] = [torch.cat(s,0) for s in list(zip(*actions_memory)) ]\n",
    "\n",
    "print('loaded and merged data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wnwwnw  Constant pred baseline:\n",
      "tensor(7.6640)\n",
      "llllll  Constant pred baseline:\n",
      "tensor(12.5311)\n",
      "llwwll  Constant pred baseline:\n",
      "tensor(12.6220)\n",
      "[[ 7.66399145 12.53108692 12.6220026 ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# compute constant prediction baseline:\n",
    "# If we were to always predict that delta_fd = 0, \n",
    "# what would the error be? Used as a baseline.\n",
    "\n",
    "results_matrix = np.matrix(np.zeros([len(condition_tuples)*3+1, len(urdf_names)]))\n",
    "\n",
    "const_pred_diff_list = dict()\n",
    "for i_urdf in range(len(urdf_names)):\n",
    "    urdf = urdf_names[i_urdf]\n",
    "    const_pred_diff_list[urdf] = []\n",
    "    for run_index in range(len(run_lens_validation[urdf])):\n",
    "\n",
    "        state_seq0 = [s[:-1] for s in states_memory_validation[urdf][run_index]]\n",
    "        state_seq1 = [s[1:] for s in states_memory_validation[urdf][run_index]]\n",
    "        action_seq = [a[:-1] for a in actions_memory_validation[urdf][run_index]]\n",
    "\n",
    "        # process states to move them to vehicle frame\n",
    "        fd_input_real, delta_fd_real = to_body_frame_batch(state_seq0, state_seq1)\n",
    "        delta_fd = torch.cat(delta_fd_real,1)\n",
    "\n",
    "        const_pred_diff_list[urdf].append(delta_fd)\n",
    "    print(urdf, ' Constant pred baseline:')\n",
    "    print( torch.abs(torch.cat(const_pred_diff_list[urdf],0)).sum(-1).mean() )\n",
    "\n",
    "    results_matrix[0,i_urdf] = torch.abs(torch.cat(const_pred_diff_list[urdf],0)).sum(-1).mean().numpy()\n",
    "\n",
    "print(results_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wnwwnw (100, 2) MLP differences:\n",
      "Mean: 7.7614006996154785\n",
      "Std: 4.886170864105225\n",
      "wnwwnw (1000, 2) MLP differences:\n",
      "Mean: 3.7145955562591553\n",
      "Std: 3.5600602626800537\n",
      "wnwwnw (5000, 2) MLP differences:\n",
      "Mean: 2.4433681964874268\n",
      "Std: 2.4891090393066406\n",
      "llllll (100, 2) MLP differences:\n",
      "Mean: 8.790816307067871\n",
      "Std: 4.93350887298584\n",
      "llllll (1000, 2) MLP differences:\n",
      "Mean: 7.702385902404785\n",
      "Std: 4.226662635803223\n",
      "llllll (5000, 2) MLP differences:\n",
      "Mean: 7.818650722503662\n",
      "Std: 4.077511787414551\n",
      "llwwll (100, 2) MLP differences:\n",
      "Mean: 9.042378425598145\n",
      "Std: 5.316173076629639\n",
      "llwwll (1000, 2) MLP differences:\n",
      "Mean: 8.545150756835938\n",
      "Std: 4.9753193855285645\n",
      "llwwll (5000, 2) MLP differences:\n",
      "Mean: 8.220766067504883\n",
      "Std: 4.607919216156006\n"
     ]
    }
   ],
   "source": [
    "# Evalute MLP accuracy\n",
    "# for urdf in urdf_names\n",
    "for i_urdf in range(len(urdf_names)):\n",
    "    urdf = urdf_names[i_urdf]\n",
    "    \n",
    "    for i_condition_tuple in range(len(condition_tuples)):\n",
    "        condition_tuple = condition_tuples[i_condition_tuple]\n",
    "        seq_len = condition_tuple[1]\n",
    "        n_rollouts_to_use = condition_tuple[0]\n",
    "    \n",
    "        \n",
    "\n",
    "        # load network\n",
    "#         PATH = os.path.join(cwd,'learned_models',\n",
    "        PATH = os.path.join(cwd,'learned_models/longer_training_3des',\n",
    "                            urdf + '_pmlp_r' +\n",
    "                            str(int(n_rollouts_to_use)) + \n",
    "                            '_ms'+ str(int(seq_len))+'.pt')\n",
    "\n",
    "        if os.path.exists(PATH):\n",
    "            save_dict = torch.load( PATH)#, map_location=lambda storage, loc: storage)\n",
    "            input_len, output_len = save_dict['fd_network_input_len'], save_dict['fd_network_output_len']\n",
    "            n_hidden_layers = save_dict['fd_network_n_hidden_layers']\n",
    "            hidden_layer_size = save_dict['fd_network_hidden_layer_size']\n",
    "\n",
    "            fd_network = pmlp(input_len = input_len, output_len=output_len,\n",
    "                n_hidden_layers = n_hidden_layers, hidden_layer_size=hidden_layer_size\n",
    "                ).to(device)\n",
    "            fd_network.load_state_dict(save_dict['fd_network_state_dict'])\n",
    "            fd_network.eval()\n",
    "\n",
    "\n",
    "            diff_list = []\n",
    "            diff_list_by_run = []\n",
    "            for run_index in range(len(run_lens_validation[urdf])):\n",
    "                with torch.no_grad():\n",
    "                    state_seq0 = [s[:-1] for s in states_memory_validation[urdf][run_index]]\n",
    "                    state_seq1 = [s[1:] for s in states_memory_validation[urdf][run_index]]\n",
    "                    action_seq = [a[:-1] for a in actions_memory_validation[urdf][run_index]]\n",
    "\n",
    "                    # process states to move them to vehicle frame\n",
    "                    fd_input_real, delta_fd_real = to_body_frame_batch(state_seq0, state_seq1)\n",
    "\n",
    "                    # pass through network\n",
    "                    fd_input = torch.cat(fd_input_real,1).to(device)\n",
    "                    actions_in = torch.cat(action_seq,1).to(device)\n",
    "                    delta_fd = torch.cat(delta_fd_real,1).to(device)\n",
    "                    state_delta_est_mean, state_delta_est_var = fd_network(fd_input, actions_in)\n",
    "\n",
    "                    diff = (delta_fd - state_delta_est_mean)\n",
    "                    diff_list_by_run.append(torch.abs(diff).sum(-1).mean())\n",
    "\n",
    "\n",
    "                diff_list.append(diff)\n",
    "            diff_list_all = torch.cat(diff_list,0)\n",
    "            print(urdf + ' ' + str(condition_tuple) + ' MLP differences:')\n",
    "            diffs_abs = torch.abs(diff_list_all).sum(-1)\n",
    "\n",
    "            print('Mean: ' + str(diffs_abs.mean().item() ))\n",
    "            print('Std: ' + str(diffs_abs.std().item()))\n",
    "            \n",
    "            results_matrix[i_condition_tuple+1,i_urdf] = diffs_abs.mean().item()\n",
    "#     print('MLP relative to const baseline mean:')\n",
    "#     const_pred = torch.abs(torch.cat(const_pred_diff_list[urdf],0)).sum(-1).to(device)\n",
    "#     diff_rel = diffs_abs/const_pred\n",
    "#     print('Mean: ' + str(diff_rel.mean().item() ))\n",
    "#     print('Std: ' + str(diff_rel.std().item()))\n",
    "#     # print(np.mean(diff_list/const_pred_diff_list[urdf]))\n",
    "\n",
    "#     print(urdf + ' MLP differences by run:')\n",
    "#     diffs_sums = torch.stack(diff_list_by_run,-1)\n",
    "#     print('Mean: ' + str(diffs_sums.mean().item() ))\n",
    "#     print('Std: ' + str(diffs_sums.std().item()))\n",
    "del fd_network, fd_input, actions_in, delta_fd\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "wnwwnw (100, 2) GNN differences:\n",
      "Mean: 2.528707265853882\n",
      "Std: 2.653336524963379\n",
      "------------------------------\n",
      "wnwwnw (1000, 2) GNN differences:\n",
      "Mean: 1.950215220451355\n",
      "Std: 2.1682562828063965\n",
      "------------------------------\n",
      "wnwwnw (5000, 2) GNN differences:\n",
      "Mean: 1.9581942558288574\n",
      "Std: 2.107177495956421\n",
      "------------------------------\n",
      "llllll (100, 2) GNN differences:\n",
      "Mean: 4.71076774597168\n",
      "Std: 3.483875274658203\n",
      "------------------------------\n",
      "llllll (1000, 2) GNN differences:\n",
      "Mean: 5.1610426902771\n",
      "Std: 3.322169780731201\n",
      "------------------------------\n",
      "llllll (5000, 2) GNN differences:\n",
      "Mean: 4.85939884185791\n",
      "Std: 3.070873498916626\n",
      "------------------------------\n",
      "llwwll (100, 2) GNN differences:\n",
      "Mean: 4.409536361694336\n",
      "Std: 3.644829750061035\n",
      "------------------------------\n",
      "llwwll (1000, 2) GNN differences:\n",
      "Mean: 4.543127536773682\n",
      "Std: 3.427647352218628\n",
      "------------------------------\n",
      "llwwll (5000, 2) GNN differences:\n",
      "Mean: 4.5057759284973145\n",
      "Std: 3.2577028274536133\n"
     ]
    }
   ],
   "source": [
    "# Evalute GNN accuracy for one design\n",
    "# urdf = urdf_names[0]\n",
    "for i_urdf in range(len(urdf_names)):\n",
    "    urdf = urdf_names[i_urdf]\n",
    "    \n",
    "    for i_condition_tuple in range(len(condition_tuples)):\n",
    "        condition_tuple = condition_tuples[i_condition_tuple]\n",
    "        seq_len = condition_tuple[1]\n",
    "        n_rollouts_to_use =  condition_tuple[0]\n",
    "\n",
    "        # load network\n",
    "#         PATH = os.path.join(cwd,'learned_models', \n",
    "        PATH = os.path.join(cwd,'learned_models/longer_training_3des', \n",
    "                            urdf + '_pgnn_r' + str(int(n_rollouts_to_use)) + \n",
    "                '_ms'+ str(int(seq_len))+'.pt')\n",
    "\n",
    "    #     PATH = ('learned_models/' + 'multidesign_pgnn_r' + str(int(n_rollouts_to_use)) + \n",
    "    #             '_ms'+ str(int(seq_len))+'.pt')\n",
    "\n",
    "#         print(PATH)\n",
    "\n",
    "        if os.path.exists(PATH):\n",
    "\n",
    "            save_dict = torch.load( PATH)#, map_location=lambda storage, loc: storage)\n",
    "            internal_state_len = save_dict['internal_state_len']\n",
    "            message_len= save_dict['message_len']\n",
    "            hidden_layer_size= save_dict['hidden_layer_size']\n",
    "\n",
    "            gnn_nodes = pgnn.create_GNN_nodes(internal_state_len, message_len, hidden_layer_size, \n",
    "                            device, body_input = True)\n",
    "            pgnn.load_state_dicts(gnn_nodes, save_dict['gnn_state_dicts'])\n",
    "            for gnn_node in gnn_nodes:\n",
    "                gnn_node.eval()\n",
    "\n",
    "            modules = dict()\n",
    "            modules[urdf] = []\n",
    "            n_modules = len(modules_types[urdf])\n",
    "            for i in range(n_modules):\n",
    "                modules[urdf].append(pgnn.Module(i, gnn_nodes[modules_types[urdf][i]], device))\n",
    "\n",
    "\n",
    "            diff_list = dict()\n",
    "            diff_list_by_run =  dict()\n",
    "\n",
    "            diff_list[urdf] = []\n",
    "            diff_list_by_run[urdf] = []\n",
    "            for run_index in range(len(run_lens_validation[urdf])):\n",
    "                batch_size = run_lens_validation[urdf][run_index]-1\n",
    "                with torch.no_grad():\n",
    "                    state_seq0 = [s[:-1] for s in states_memory_validation[urdf][run_index]]\n",
    "                    state_seq1 = [s[1:]  for s in states_memory_validation[urdf][run_index]]\n",
    "                    action_seq = [a[:-1] for a in actions_memory_validation[urdf][run_index]]\n",
    "\n",
    "                    for module in modules[urdf]: # must reset module lstm state\n",
    "                        module.reset_hidden_states(batch_size) \n",
    "\n",
    "                    # process states to move them to vehicle frame\n",
    "                    fd_input_real, delta_fd_real = to_body_frame_batch(state_seq0, state_seq1)\n",
    "\n",
    "                    # pass through network\n",
    "                    fd_input   = to_device(fd_input_real, device) \n",
    "                    actions_in = to_device(action_seq, device)\n",
    "                    node_inputs = [torch.cat([s,a],1) for (s,a) in zip(fd_input, actions_in)]\n",
    "                    state_delta_est_mean, state_delta_var = pgnn.run_propagations(\n",
    "                        modules[urdf], attachments[urdf], 2, node_inputs, device)\n",
    "\n",
    "                    # cat to one tensor and take difference\n",
    "                    state_delta_est_mean = torch.cat(state_delta_est_mean,-1)\n",
    "                    delta_fd   = torch.cat(to_device(delta_fd_real, device),-1)\n",
    "                    diff = (delta_fd - state_delta_est_mean)\n",
    "\n",
    "                diff_list_by_run[urdf].append(torch.abs(diff).sum(-1).mean())   \n",
    "                diff_list[urdf].append(diff)\n",
    "\n",
    "            print('------------------------------')    \n",
    "            diff_list_all = torch.cat(diff_list[urdf],0)\n",
    "            print(urdf + ' ' + str(condition_tuple)+ ' GNN differences:')\n",
    "            diffs_abs = torch.abs(diff_list_all).sum(-1)\n",
    "\n",
    "            print('Mean: ' + str(diffs_abs.mean().item() ))\n",
    "            print('Std: ' + str(diffs_abs.std().item()))\n",
    "            \n",
    "            results_matrix[i_condition_tuple+1+3,i_urdf] = diffs_abs.mean().item()\n",
    "\n",
    "#         print(urdf + ' GNN relative to const baseline mean:')\n",
    "#         const_pred = torch.abs(torch.cat(const_pred_diff_list[urdf],0)).sum(-1).to(device)\n",
    "#         diff_rel = diffs_abs/const_pred\n",
    "#         print('Mean: ' + str(diff_rel.mean().item() ))\n",
    "#         print('Std: ' + str(diff_rel.std().item()))\n",
    "\n",
    "#         print(urdf + ' GNN differences by run:')\n",
    "#         diffs_sums = torch.stack(diff_list_by_run[urdf],-1)\n",
    "#         print('Mean: ' + str(diffs_sums.mean().item() ))\n",
    "#         print('Std: ' + str(diffs_sums.std().item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned_models/longer_training/multidesign_pgnn_r100_ms2.pt\n",
      "------------------------------\n",
      "wnwwnw GNN differences:\n",
      "Mean: 2.4028401374816895\n",
      "Std: 2.749899387359619\n",
      "------------------------------\n",
      "llllll GNN differences:\n",
      "Mean: 5.068259239196777\n",
      "Std: 3.53294038772583\n",
      "------------------------------\n",
      "llwwll GNN differences:\n",
      "Mean: 6.886737823486328\n",
      "Std: 3.877237558364868\n",
      "learned_models/longer_training/multidesign_pgnn_r1000_ms2.pt\n",
      "------------------------------\n",
      "wnwwnw GNN differences:\n",
      "Mean: 2.0955729484558105\n",
      "Std: 2.3948006629943848\n",
      "------------------------------\n",
      "llllll GNN differences:\n",
      "Mean: 5.254051685333252\n",
      "Std: 3.3526487350463867\n",
      "------------------------------\n",
      "llwwll GNN differences:\n",
      "Mean: 5.347886562347412\n",
      "Std: 3.459967613220215\n",
      "learned_models/longer_training/multidesign_pgnn_r5000_ms2.pt\n",
      "------------------------------\n",
      "wnwwnw GNN differences:\n",
      "Mean: 2.239934206008911\n",
      "Std: 2.4226791858673096\n",
      "------------------------------\n",
      "llllll GNN differences:\n",
      "Mean: 5.303031921386719\n",
      "Std: 3.2957000732421875\n",
      "------------------------------\n",
      "llwwll GNN differences:\n",
      "Mean: 6.162678241729736\n",
      "Std: 3.508631706237793\n"
     ]
    }
   ],
   "source": [
    "# Evalute GNN accuracy for all designs\n",
    "for i_condition_tuple in range(len(condition_tuples)):\n",
    "    condition_tuple = condition_tuples[i_condition_tuple]\n",
    "    seq_len = condition_tuple[1]\n",
    "    n_rollouts_to_use = condition_tuple[0]\n",
    "    \n",
    "    # load network\n",
    "\n",
    "#     PATH = os.path.join(cwd,'learned_models', \n",
    "    PATH = os.path.join(cwd,'learned_models/longer_training', \n",
    "                        'multidesign_pgnn_r' + str(int(n_rollouts_to_use)) + \n",
    "            '_ms'+ str(int(seq_len))+'.pt')\n",
    "\n",
    "    print(PATH)\n",
    "    save_dict = torch.load( PATH)#, map_location=lambda storage, loc: storage)\n",
    "    internal_state_len = save_dict['internal_state_len']\n",
    "    message_len= save_dict['message_len']\n",
    "    hidden_layer_size= save_dict['hidden_layer_size']\n",
    "\n",
    "    gnn_nodes = pgnn.create_GNN_nodes(internal_state_len, message_len, hidden_layer_size, \n",
    "                    device, body_input = True)\n",
    "    pgnn.load_state_dicts(gnn_nodes, save_dict['gnn_state_dicts'])\n",
    "    for gnn_node in gnn_nodes:\n",
    "        gnn_node.eval()\n",
    "\n",
    "        \n",
    "    modules = dict()\n",
    "    for urdf in urdf_names:\n",
    "        modules[urdf] = []\n",
    "        n_modules = len(modules_types[urdf])\n",
    "        for i in range(n_modules):\n",
    "            modules[urdf].append(pgnn.Module(i, gnn_nodes[modules_types[urdf][i]], device))\n",
    "\n",
    "\n",
    "    diff_list = dict()\n",
    "    diff_list_by_run =  dict()\n",
    "\n",
    "    for i_urdf in range(len(urdf_names)):\n",
    "        urdf = urdf_names[i_urdf]\n",
    "        \n",
    "        diff_list[urdf] = []\n",
    "        diff_list_by_run[urdf] = []\n",
    "        for run_index in range(len(run_lens_validation[urdf])):\n",
    "            batch_size = run_lens_validation[urdf][run_index]-1\n",
    "            with torch.no_grad():\n",
    "                state_seq0 = [s[:-1] for s in states_memory_validation[urdf][run_index]]\n",
    "                state_seq1 = [s[1:]  for s in states_memory_validation[urdf][run_index]]\n",
    "                action_seq = [a[:-1] for a in actions_memory_validation[urdf][run_index]]\n",
    "\n",
    "                for module in modules[urdf]: # must reset module lstm state\n",
    "                    module.reset_hidden_states(batch_size) \n",
    "\n",
    "                # process states to move them to vehicle frame\n",
    "                fd_input_real, delta_fd_real = to_body_frame_batch(state_seq0, state_seq1)\n",
    "\n",
    "                # pass through network\n",
    "                fd_input   = to_device(fd_input_real, device) \n",
    "                actions_in = to_device(action_seq, device)\n",
    "                node_inputs = [torch.cat([s,a],1) for (s,a) in zip(fd_input, actions_in)]\n",
    "                state_delta_est_mean, state_delta_var = pgnn.run_propagations(\n",
    "                    modules[urdf], attachments[urdf], 2, node_inputs, device)\n",
    "\n",
    "                # cat to one tensor and take difference\n",
    "                state_delta_est_mean = torch.cat(state_delta_est_mean,-1)\n",
    "                delta_fd   = torch.cat(to_device(delta_fd_real, device),-1)\n",
    "                diff = (delta_fd - state_delta_est_mean)\n",
    "\n",
    "            diff_list_by_run[urdf].append(torch.abs(diff).sum(-1).mean())   \n",
    "            diff_list[urdf].append(diff)\n",
    "\n",
    "        print('------------------------------')    \n",
    "        diff_list_all = torch.cat(diff_list[urdf],0)\n",
    "        print(urdf + ' GNN differences:')\n",
    "        diffs_abs = torch.abs(diff_list_all).sum(-1)\n",
    "\n",
    "        print('Mean: ' + str(diffs_abs.mean().item() ))\n",
    "        print('Std: ' + str(diffs_abs.std().item()))\n",
    "        \n",
    "        results_matrix[i_condition_tuple+1+3+3,i_urdf] = diffs_abs.mean().item()\n",
    "\n",
    "#         print(urdf + ' GNN relative to const baseline mean:')\n",
    "#         const_pred = torch.abs(torch.cat(const_pred_diff_list[urdf],0)).sum(-1).to(device)\n",
    "#         diff_rel = diffs_abs/const_pred\n",
    "#         print('Mean: ' + str(diff_rel.mean().item() ))\n",
    "#         print('Std: ' + str(diff_rel.std().item()))\n",
    "\n",
    "#         print(urdf + ' GNN differences by run:')\n",
    "#         diffs_sums = torch.stack(diff_list_by_run[urdf],-1)\n",
    "#         print('Mean: ' + str(diffs_sums.mean().item() ))\n",
    "#         print('Std: ' + str(diffs_sums.std().item()))\n",
    "\n",
    "print(results_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llllll using 100 out of Rollouts 5000\n",
      "tensor([[1220, 6710, 4214, 6770, 3124, 4047, 1486,  426, 4785, 6434],\n",
      "        [1221, 6711, 4215, 6771, 3125, 4048, 1487,  427, 4786, 6435]])\n"
     ]
    }
   ],
   "source": [
    "# condition_tuple = condition_tuples[0]\n",
    "# n_rollouts_to_use = condition_tuple[0]\n",
    "# seq_len = condition_tuple[1]\n",
    "# # depending on the length of the multistep sequence we want,\n",
    "# # only some indexes of the full set of states collected can be sampled.\n",
    "# sampleable_inds = dict()\n",
    "# batch_sizes = dict()\n",
    "# #     for urdf in urdf_names:\n",
    "# sampleable_inds[urdf] = get_sampleable_inds(run_lens[urdf][:n_rollouts_to_use], seq_len)\n",
    "# n_sampleable = len(sampleable_inds[urdf])\n",
    "# batch_sizes[urdf] = batch_size_default\n",
    "# if batch_sizes[urdf] > n_sampleable:\n",
    "#     batch_sizes[urdf] = n_sampleable\n",
    "# print(urdf + ' using ' + str(n_rollouts_to_use) + ' out of Rollouts ' + str(len(run_lens[urdf])))\n",
    "\n",
    "\n",
    "# sampled_inds = sampleable_inds[urdf][np.random.choice(len(sampleable_inds[urdf]), 10, replace=False)]\n",
    "# sampled_ranges = sampled_inds.repeat((seq_len,1))\n",
    "# for si in range(seq_len):\n",
    "#     sampled_ranges[si] += si\n",
    "# print(sampled_ranges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
