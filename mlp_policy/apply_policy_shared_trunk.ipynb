{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144  designs\n",
      "12  seen designs\n",
      "132  unseen designs\n"
     ]
    }
   ],
   "source": [
    "# Source code for paper \"Learning modular robot control policies\" in Transactions on Robotics\n",
    "# MLP comparisons\n",
    "# Julian Whitman, Dec. 2022. \n",
    "## Apply the shared trunk policy. It can only apply to designs seen during training. \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from robot_env import robot_env\n",
    "from utils import to_tensors, combine_state, wrap_to_pi, rotate\n",
    "import os, sys\n",
    "sys.path.insert(0,'..') # print_xacros and urdfs are in the parent directory\n",
    "from print_xacros import get_names, compile_to_urdf\n",
    "from apply_policy_MLP import apply_policy, make_goal_memory\n",
    "from planning_utils import compare_velocities\n",
    "\n",
    "all_name_list = get_names()\n",
    "print(len(all_name_list), ' designs')\n",
    "\n",
    "# divide them up into seen and unseen\n",
    "unseen_inds = []\n",
    "seen_inds = []\n",
    "for i_urdf in range(len(all_name_list)):\n",
    "    urdf = all_name_list[i_urdf]\n",
    "    if (urdf[0:3]==urdf[3:6][::-1]):\n",
    "        seen_inds.append(i_urdf)\n",
    "    else:\n",
    "        unseen_inds.append(i_urdf)\n",
    "print(len(seen_inds), ' seen designs')\n",
    "print(len(unseen_inds), ' unseen designs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from saved/shared_trunk_tripod1/shared_trunk_control_iter3.pt\n",
      "fd_input_lens_sums, action_lens_sums, policy_input_lens_sums,fd_output_lens_sums: [45, 27, 39, 33, 39, 33, 33, 33, 27, 27, 39, 21], [18, 10, 16, 12, 16, 14, 14, 14, 12, 10, 16, 8], [41, 23, 35, 29, 35, 29, 29, 29, 23, 23, 35, 17], [48, 30, 42, 36, 42, 36, 36, 36, 30, 30, 42, 24]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared_MLP_utils import get_in_out_lens\n",
    "from shared_MLP_policy import shared_trunk_policy\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# make some direction goals\n",
    "goal_memory = make_goal_memory(41) # 10*4 + 1\n",
    "\n",
    "# load up a learned policy to test\n",
    "folder = 'saved/shared_trunk_tripod1' \n",
    "control_fname = os.path.join(folder, 'shared_trunk_control_iter3.pt')\n",
    "\n",
    "print('Loading weights from ' + control_fname)\n",
    "save_dict = torch.load( control_fname, map_location=lambda storage, loc: storage)\n",
    "urdf_names = save_dict['urdf_names']\n",
    "fd_input_lens, fd_output_lens, policy_input_lens,action_lens,limb_types = get_in_out_lens(urdf_names)\n",
    "\n",
    "fd_input_lens_sums = [sum(s) for s in fd_input_lens]\n",
    "fd_output_lens_sums = [sum(s) for s in fd_output_lens]\n",
    "action_lens_sums = [sum(a) for a in action_lens]\n",
    "policy_input_lens_sums = [sum(s) for s in policy_input_lens]\n",
    "print('fd_input_lens_sums, action_lens_sums, policy_input_lens_sums,fd_output_lens_sums: ' + \n",
    "    str(fd_input_lens_sums) + ', ' +\n",
    "    str(action_lens_sums) +', ' +\n",
    "    str(policy_input_lens_sums) +', ' +\n",
    "    str(fd_output_lens_sums))\n",
    "\n",
    "state_dict= save_dict['state_dict'] \n",
    "n_hidden_layers = save_dict['n_hidden_layers'] \n",
    "hidden_layer_size = save_dict['hidden_layer_size']\n",
    "goal_len =3\n",
    "\n",
    "print(save_dict['comment'])\n",
    "\n",
    "# env.reset_robot(urdf_name=urdf_name, randomize_start=False)\n",
    "# attachments = env.attachments\n",
    "# modules_types = env.modules_types\n",
    "# print('attachments: ' + str(attachments))\n",
    "# print('modules_types: ' + str(modules_types))\n",
    "# n_modules = len(modules_types)\n",
    "\n",
    "# env_state_init = env.get_state()\n",
    "# module_state_len = []\n",
    "# for s in env_state_init:\n",
    "#     module_state_len.append(len(s))\n",
    "\n",
    "# state_len= np.sum(module_state_len)\n",
    "# action_len = env.num_joints\n",
    "# module_action_len = list(np.diff(env.action_indexes))\n",
    "\n",
    "# module_sa_len = module_state_len+ module_action_len\n",
    "\n",
    "n_hidden_layers\n",
    "policy_network = shared_trunk_policy(\n",
    "    policy_input_lens_sums, action_lens_sums, \n",
    "    goal_len, n_hidden_layers, hidden_layer_size)\n",
    "\n",
    "policy_network.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llllll', 'lnwwnl', 'llwwll', 'lnllnl', 'lwllwl', 'lwwwwl', 'wlwwlw', 'wwllww', 'wwwwww', 'wnllnw', 'wllllw', 'wnwwnw']\n",
      "Created folder saved/shared_trunk_tripod1/transfer_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Note these must be in the same order as seen in training, since they are stored by index\n",
    "urdf_names = ['llllll', 'lnwwnl', 'llwwll', 'lnllnl', \n",
    "              'lwllwl', 'lwwwwl', 'wlwwlw', 'wwllww', \n",
    "              'wwwwww', 'wnllnw', 'wllllw', 'wnwwnw']\n",
    "print(urdf_names)\n",
    "\n",
    "\n",
    "\n",
    "data_subfolder = os.path.join(folder,'transfer_data')\n",
    "\n",
    "if not(os.path.exists(data_subfolder)):\n",
    "    os.mkdir(data_subfolder)\n",
    "    print('Created folder ' + data_subfolder)\n",
    "else:\n",
    "    print('Using folder ' + data_subfolder)\n",
    "    \n",
    "\n",
    "\n",
    "CREATE_VIDEOS= False\n",
    "# CREATE_VIDEOS= True\n",
    "for i_urdf in range(len(urdf_names)):\n",
    "    urdf_name = urdf_names[i_urdf]\n",
    "    urdf_file =  os.path.join(os.path.split(os.getcwd())[0], 'urdf/' + urdf_name  + '.urdf')\n",
    "    if not(os.path.exists(urdf_file)):\n",
    "        compile_to_urdf(urdf_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llllll: 0.12 baseline 0.12 0/12\n",
      "lnwwnl: 0.08 baseline 0.12 1/12\n",
      "llwwll: 0.09 baseline 0.12 2/12\n",
      "lnllnl: 0.1 baseline 0.12 3/12\n",
      "lwllwl: 0.1 baseline 0.12 4/12\n",
      "lwwwwl: 0.08 baseline 0.12 5/12\n",
      "wlwwlw: 0.07 baseline 0.12 6/12\n",
      "wwllww: 0.1 baseline 0.12 7/12\n",
      "wwwwww: 0.06 baseline 0.12 8/12\n",
      "wnllnw: 0.09 baseline 0.12 9/12\n",
      "wllllw: 0.1 baseline 0.12 10/12\n",
      "wnwwnw: 0.04 baseline 0.12 11/12\n"
     ]
    }
   ],
   "source": [
    "T = 20\n",
    "vel_metric_list = []\n",
    "vel_baseline_list = []\n",
    "video_names = []\n",
    "for i_urdf in range(len(urdf_names)):\n",
    "    urdf = urdf_names[i_urdf]\n",
    "\n",
    "    save_path = os.path.join(data_subfolder, urdf + '_apply_policy.ptx')\n",
    "    video_name = os.path.join(data_subfolder, urdf + '_goal')\n",
    "    video_names.append(video_name)\n",
    "\n",
    "    apply_policy(urdf, i_urdf, goal_memory, \n",
    "                 policy_network, device, save_path, show_GUI=False)\n",
    "\n",
    "    save_dict = torch.load(save_path, map_location=lambda storage, loc: storage)\n",
    "    vel_metric, vel_metric_baseline = compare_velocities(\n",
    "            save_dict['states_memory'],\n",
    "            save_dict['goal_memory'], \n",
    "            save_dict['run_lens'],\n",
    "            10, T )\n",
    "    vel_metric_list.append(vel_metric)\n",
    "    vel_baseline_list.append(vel_metric_baseline)\n",
    "    print(urdf + ': ' + str(np.round(vel_metric,2))\n",
    "          + ' baseline ' + str(np.round(vel_metric_baseline,2))\n",
    "          + ' ' + str(i_urdf) + '/' + str(len(urdf_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote file  saved/shared_trunk_tripod1/transfer_data/transfer_results.ptx\n"
     ]
    }
   ],
   "source": [
    "seen_names = urdf_names\n",
    "if True:\n",
    "# if len(urdf_names)>50:\n",
    "    vel_data_path = os.path.join(data_subfolder, 'transfer_results.ptx')\n",
    "    vel_dict = dict()\n",
    "#     vel_dict['unseen_inds'] = unseen_inds\n",
    "#     vel_dict['seen_inds'] = seen_inds\n",
    "    vel_dict['urdf_names'] = urdf_names\n",
    "    vel_dict['vel_metric_list'] = vel_metric_list\n",
    "    vel_dict['vel_baseline_list'] = vel_baseline_list\n",
    "    torch.save(vel_dict, vel_data_path)\n",
    "\n",
    "    vel_save_path = os.path.join(data_subfolder, 'transfer_results.csv')\n",
    "#     seen_names = [urdf_names[s] for s in seen_inds]\n",
    "#     unseen_names = [urdf_names[s] for s in unseen_inds]\n",
    "\n",
    "    vel_metric_list = np.array(vel_metric_list)\n",
    "    vel_baseline_list = np.array(vel_baseline_list)\n",
    "\n",
    "    with open(vel_save_path, 'w') as fp:\n",
    "        names_text = ''\n",
    "#         for urdf in unseen_names:\n",
    "#             names_text = names_text + urdf + ',' \n",
    "\n",
    "#         fp.write('--- Unseen Names: ---\\n')\n",
    "#         fp.write(names_text + '\\n')\n",
    "#         fp.write('Metric Mean: ' + str(np.mean(vel_metric_list[unseen_inds]))+'\\n')\n",
    "#         fp.write('Metric Min: ' + str(np.min(vel_metric_list[unseen_inds]))+'\\n')\n",
    "#         fp.write('Metric Max: ' + str(np.max(vel_metric_list[unseen_inds]))+'\\n')\n",
    "#         fp.write('Metric Rescaled: ' + str(\n",
    "#             np.mean( (vel_baseline_list[unseen_inds] - vel_metric_list[unseen_inds])\n",
    "#                     /vel_baseline_list[unseen_inds])\n",
    "#             )+'\\n')\n",
    "\n",
    "#         fp.write('--- Metric: ---\\n')\n",
    "#         np.savetxt(fp, vel_metric_list[unseen_inds], delimiter=',')   \n",
    "#         fp.write('--- Baseline: ---\\n')\n",
    "#         np.savetxt(fp, vel_baseline_list[unseen_inds], delimiter=',')\n",
    "\n",
    "        names_text = ''\n",
    "        for urdf in seen_names:\n",
    "            names_text = names_text + urdf + ',' \n",
    "\n",
    "        fp.write('--- Seen Names: ---\\n')\n",
    "        fp.write(names_text + '\\n')\n",
    "        fp.write('Metric Mean: ' + str(np.mean(vel_metric_list))+'\\n')\n",
    "        fp.write('Metric Min: ' + str(np.min(vel_metric_list))+'\\n')\n",
    "        fp.write('Metric Max: ' + str(np.max(vel_metric_list))+'\\n')\n",
    "        fp.write('Metric Rescaled: ' + str(\n",
    "            np.mean( (vel_baseline_list - vel_metric_list)\n",
    "                    /vel_baseline_list)\n",
    "            )+'\\n')\n",
    "        fp.write('--- Metric: ---\\n')\n",
    "        np.savetxt(fp, vel_metric_list, delimiter=',')   \n",
    "        fp.write('--- Baseline: ---\\n')\n",
    "        np.savetxt(fp, vel_baseline_list, delimiter=',')\n",
    "        print('wrote file  ' + vel_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
