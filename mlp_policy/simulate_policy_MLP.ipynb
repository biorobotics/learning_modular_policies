{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Logitech Logitech Dual Action\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Source code for paper \"Learning modular robot control policies\" in Transactions on Robotics\n",
    "MLP comparisons\n",
    "Julian Whitman, Dec. 2022. \n",
    "\n",
    "Run MLP policy on robot. Currently set to shared_trunk policy, could be changed to hardware_conditioned.\n",
    "The control inputs stripped to frame with (x y yaw) removed\n",
    "Uses the trained policy from the velocity input\n",
    "\n",
    "Uses previous observation as input for next action\n",
    "\n",
    "'''\n",
    "\n",
    "# load libraries\n",
    "import torch\n",
    "from robot_env import robot_env\n",
    "import numpy as np\n",
    "import os\n",
    "import pgnn_control as pgnnc\n",
    "from utils import to_tensors, combine_state, wrap_to_pi, rotate, create_control_inputs\n",
    "from shared_MLP_policy import shared_trunk_policy\n",
    "from shared_MLP_utils import get_in_out_lens\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# init env\n",
    "# use the env to get the size of the inputs and outputs\n",
    "env = robot_env(show_GUI = True)\n",
    "env.reset_terrain()\n",
    "\n",
    "    \n",
    "# USE_JOY = True\n",
    "USE_JOY = False\n",
    "if USE_JOY:\n",
    "    from vrjoystick import init_joystick, read\n",
    "    joy = init_joystick()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['llllll', 'lwllwl', 'wnwwnw']\n",
      "fd_input_lens_sums, action_lens_sums, policy_input_lens_sums,fd_output_lens_sums: [45, 39, 21], [18, 16, 8], [41, 35, 17], [48, 42, 24]\n",
      "\n",
      "attachments: [[1, 2, 3, 4, 5, 6], [0], [0], [0], [0], [0], [0]]\n",
      "modules_types: [0, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## select which policy file to load\n",
    "folder = 'saved/mbrl_shared_trunk1'\n",
    "PATH =  'shared_trunk_control_iter3.pt'\n",
    "PATH = os.path.join(folder,PATH)\n",
    "\n",
    "save_dict = torch.load( PATH, map_location=lambda storage, loc: storage)\n",
    "urdf_names = save_dict['urdf_names']\n",
    "print(urdf_names)\n",
    "# select which robot to put in\n",
    "urdf_name = 'wnwwnw'\n",
    "urdf_name = 'llllll'\n",
    "\n",
    "fd_input_lens, fd_output_lens, policy_input_lens,action_lens,limb_types = get_in_out_lens(urdf_names)\n",
    "\n",
    "fd_input_lens_sums = [sum(s) for s in fd_input_lens]\n",
    "fd_output_lens_sums = [sum(s) for s in fd_output_lens]\n",
    "action_lens_sums = [sum(a) for a in action_lens]\n",
    "policy_input_lens_sums = [sum(s) for s in policy_input_lens]\n",
    "print('fd_input_lens_sums, action_lens_sums, policy_input_lens_sums,fd_output_lens_sums: ' + \n",
    "    str(fd_input_lens_sums) + ', ' +\n",
    "    str(action_lens_sums) +', ' +\n",
    "    str(policy_input_lens_sums) +', ' +\n",
    "    str(fd_output_lens_sums))\n",
    "\n",
    "state_dict= save_dict['state_dict'] \n",
    "n_hidden_layers = save_dict['n_hidden_layers'] \n",
    "hidden_layer_size = save_dict['hidden_layer_size']\n",
    "goal_len =3\n",
    "\n",
    "print(save_dict['comment'])\n",
    "\n",
    "env.reset_robot(urdf_name=urdf_name, randomize_start=False)\n",
    "\n",
    "attachments = env.attachments\n",
    "modules_types = env.modules_types\n",
    "print('attachments: ' + str(attachments))\n",
    "print('modules_types: ' + str(modules_types))\n",
    "n_modules = len(modules_types)\n",
    "\n",
    "env_state_init = env.get_state()\n",
    "module_state_len = []\n",
    "for s in env_state_init:\n",
    "    module_state_len.append(len(s))\n",
    "\n",
    "state_len= np.sum(module_state_len)\n",
    "action_len = env.num_joints\n",
    "module_action_len = list(np.diff(env.action_indexes))\n",
    "\n",
    "module_sa_len = module_state_len+ module_action_len\n",
    "\n",
    "\n",
    "policy_network = shared_trunk_policy(\n",
    "    policy_input_lens_sums, action_lens_sums, \n",
    "    goal_len, n_hidden_layers, hidden_layer_size) # Change this to init a hardware conditioned if needed, \n",
    "# see apply_policy_hardware_conditioned notebook for usage\n",
    "\n",
    "policy_network.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-32080a2a28a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresetDebugVisualizerCamera\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m65\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphysicsClientId\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysicsClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# env.p.resetDebugVisualizerCamera(2.1,0,-89.999,[0,0,0.2],physicsClientId=env.physicsClient)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_robot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murdf_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murdf_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandomize_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_xyyaw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvid_fname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steer_vel.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env.p.resetDebugVisualizerCamera(2.1,0,-65,[0,0,0.2],physicsClientId=env.physicsClient) \n",
    "# env.p.resetDebugVisualizerCamera(2.1,0,-89.999,[0,0,0.2],physicsClientId=env.physicsClient) \n",
    "env.reset_robot(urdf_name=urdf_name, randomize_start=False, start_xyyaw=[0,0,0])\n",
    "\n",
    "# vid_fname='steer_vel.mp4'\n",
    "# env.start_video_log(fileName=vid_fname)\n",
    "\n",
    "# for step in range(150):\n",
    "robot_button_mapping = [0,1,2,3,4,8,9,10]\n",
    "n_buttons_to_check = len(robot_button_mapping)\n",
    "from planning_utils import speed_scale_xy, speed_scale_yaw\n",
    "# vxy_scale = 20*(20/240)*0.314/0.75\n",
    "# vyaw_scale = 20*(20/240)*1.1/0.75\n",
    "vxy_scale=speed_scale_xy\n",
    "vyaw_scale=speed_scale_yaw\n",
    "\n",
    "env_state = env.get_state()\n",
    "last_states = [smm.to(device) for smm in to_tensors(env_state)]\n",
    "# last_action = np.zeros(action_len)\n",
    "tau_list = []\n",
    "u_list = []\n",
    "design_index = urdf_names.index(urdf_name)\n",
    "buttons = np.zeros(12)\n",
    "for step in range(10000):\n",
    "    buttons = np.zeros(12)\n",
    "    if USE_JOY:\n",
    "        axes, buttons, povs = read(joy)\n",
    "        axes = np.array(axes)\n",
    "        axes[np.abs(axes)<0.01] = 0\n",
    "        desired_xyyaw = np.array([-axes[1]*vxy_scale,\n",
    "                                  -axes[0]*vxy_scale,\n",
    "                                  -axes[2]*vyaw_scale])\n",
    "        if buttons[9]==1 or buttons[8]==1:\n",
    "            break\n",
    "        \n",
    "        if np.any(buttons[0:n_buttons_to_check]) or buttons[10] or buttons[11]:\n",
    "            if np.any(buttons[0:n_buttons_to_check]):\n",
    "                pressed = np.where(buttons[0:n_buttons_to_check])[0][0]\n",
    "                urdf_name = urdf_names[robot_button_mapping[pressed]]\n",
    "            else:\n",
    "                ind = urdf_names.index(urdf_name)\n",
    "                urdf_name = urdf_names[(ind+1) % len(urdf_names)]\n",
    "\n",
    "                env.reset_robot(urdf_name=urdf_name, randomize_start=False, start_xyyaw=[0,0,0])\n",
    "            attachments = env.attachments\n",
    "            modules_types = env.modules_types\n",
    "            n_modules = len(modules_types)\n",
    "            env_state_init = env.get_state()\n",
    "            module_state_len = []\n",
    "            for s in env_state_init:\n",
    "                module_state_len.append(len(s))\n",
    "            state_len= np.sum(module_state_len)\n",
    "            action_len = env.num_joints\n",
    "            module_action_len = list(np.diff(env.action_indexes))\n",
    "            module_sa_len = module_state_len+ module_action_len\n",
    "            \n",
    "            tau_list = []\n",
    "            u_list = []\n",
    "            last_states = [smm.to(device) for smm in to_tensors(env_state_init)]\n",
    "            buttons = np.zeros(12)\n",
    "            design_index = urdf_names.index(urdf_name)\n",
    "            print(buttons)\n",
    "\n",
    "    else:\n",
    "        desired_xyyaw =  np.array([1,0, 0])\n",
    "\n",
    "        \n",
    "    chassis_yaw = env.pos_rpy[-1]\n",
    "    vect1 = np.array([desired_xyyaw[0]*np.cos(chassis_yaw) - desired_xyyaw[1]*np.sin(chassis_yaw),\n",
    "            desired_xyyaw[0]*np.sin(chassis_yaw) + desired_xyyaw[1]*np.cos(chassis_yaw),\n",
    "            0] )\n",
    "    vect2 = np.array([np.cos(desired_xyyaw[2]/2+chassis_yaw),\n",
    "                      np.sin(desired_xyyaw[2]/2+chassis_yaw), \n",
    "                      0])*np.abs(desired_xyyaw[2])\n",
    "    env.draw_body_arrows([vect1*0.5/vxy_scale, \n",
    "                          0.5*vect2/vyaw_scale],\n",
    "                         [[0,0,0], [0,0,1]])\n",
    "\n",
    "    \n",
    "    env_state = env.get_state()\n",
    "    states = [smm.to(device) for smm in to_tensors(env_state)]\n",
    "    \n",
    "\n",
    "    # heading and yaw here are wrt body frame\n",
    "    goals = torch.tensor(desired_xyyaw, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    inputs, goals = create_control_inputs(last_states, goals, rotate_goals = False)\n",
    "    inputs = torch.cat(inputs, -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        u_mean, u_var, tau_mean, tau_var = policy_network(\n",
    "                inputs, goals, design_index)\n",
    "        action = u_mean.squeeze().cpu().numpy()\n",
    "        tau =  tau_mean.squeeze().cpu().numpy()\n",
    "        tau_list.append(tau)\n",
    "        u_list.append(action)\n",
    "    \n",
    "    env.step(action)\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "    if np.dot([0,0,1], env.z_axis)<0:\n",
    "        env_yaw = env.pos_rpy[-1]\n",
    "        env.reset_robot(urdf_name=urdf_name, randomize_start=True)\n",
    "        env_state = env.get_state()\n",
    "        states = [smm.to(device) for smm in to_tensors(env_state)]\n",
    "        \n",
    "    if np.sqrt(env.pos_xyz[0]**2+ env.pos_xyz[1]**2)>2:\n",
    "        env_state = env.get_state()\n",
    "        env_state[0][0:2] = 0 \n",
    "        env.set_state(env_state)\n",
    "    \n",
    "    last_states = [s.clone() for s in states]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
